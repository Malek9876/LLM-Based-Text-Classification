{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Malek9876/LLM-Based-Text-Classification/blob/main/Sentiment_Analysis_Using_Naive_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMQV0xV5GUQX",
        "outputId": "808358cf-d3fa-4d51-a1b6-1c70457d0df4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (24.3.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: kagglehub in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.3.6)\n",
            "Requirement already satisfied: packaging in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kagglehub) (23.2)\n",
            "Requirement already satisfied: requests in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->kagglehub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->kagglehub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->kagglehub) (2024.8.30)\n",
            "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: pandas in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
            "Requirement already satisfied: numpy in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.1.1)\n",
            "Requirement already satisfied: nltk in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.6.1)\n",
            "Requirement already satisfied: transformers in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.48.0)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
            "Requirement already satisfied: seaborn in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: click in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.12.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (75.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (2.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "%pip install --upgrade pip\n",
        "%pip install kagglehub\n",
        "import kagglehub\n",
        "path = kagglehub.dataset_download('abhi8923shriv/sentiment-analysis-dataset')\n",
        "\n",
        "# Install the required libraries\n",
        "%pip install pandas numpy nltk scikit-learn transformers matplotlib seaborn\n",
        "#%pip uninstall torch torchvision torchaudio -y\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WspHhyP757H4"
      },
      "source": [
        "# Sentiment Analysis on Kaggle sentiment analysis dataset\n",
        "sentiment analysis tasks on kaggle sentiment analysis dataset using simple machine learning model: Naive bayes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgtBS7we9zxF"
      },
      "source": [
        "## Including needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-12T08:22:36.548968Z",
          "iopub.status.busy": "2024-09-12T08:22:36.548571Z",
          "iopub.status.idle": "2024-09-12T08:22:39.953833Z",
          "shell.execute_reply": "2024-09-12T08:22:39.952412Z",
          "shell.execute_reply.started": "2024-09-12T08:22:36.548927Z"
        },
        "id": "V56eXYTz3Wi6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# --------------- MAIN LIBRARIES ------------------\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# --------------- HELPING LIBRARIES ----------------\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# ------------- Pytorch Librairies ---------------\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vX8kWFB-_ou8"
      },
      "source": [
        "## Uploading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-09-12T08:22:39.957158Z",
          "iopub.status.busy": "2024-09-12T08:22:39.956494Z",
          "iopub.status.idle": "2024-09-12T08:22:39.976107Z",
          "shell.execute_reply": "2024-09-12T08:22:39.974235Z",
          "shell.execute_reply.started": "2024-09-12T08:22:39.957103Z"
        },
        "id": "K9dAjDTZBSKW",
        "outputId": "2a1964af-5ae9-4781-a91d-ffb39be37cb8",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "train_dataset = path+'/train.csv'\n",
        "test_dataset = path+'/test.csv'\n",
        "\n",
        "# Check if the path exists\n",
        "print (os.path.exists(train_dataset))\n",
        "print (os.path.exists(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-12T08:22:39.978592Z",
          "iopub.status.busy": "2024-09-12T08:22:39.978053Z",
          "iopub.status.idle": "2024-09-12T08:22:40.231514Z",
          "shell.execute_reply": "2024-09-12T08:22:40.230228Z",
          "shell.execute_reply.started": "2024-09-12T08:22:39.978535Z"
        },
        "id": "hW9BRdvvB5-i",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Load the CSV file into a DataFrame\n",
        "train_df = pd.read_csv(train_dataset, encoding='ISO-8859-1')\n",
        "test_df = pd.read_csv(test_dataset, encoding='ISO-8859-1')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "execution": {
          "iopub.execute_input": "2024-09-12T08:22:40.236134Z",
          "iopub.status.busy": "2024-09-12T08:22:40.234876Z",
          "iopub.status.idle": "2024-09-12T08:22:40.279444Z",
          "shell.execute_reply": "2024-09-12T08:22:40.278232Z",
          "shell.execute_reply.started": "2024-09-12T08:22:40.236076Z"
        },
        "id": "Cv2hsR9aDAkQ",
        "outputId": "d2e36aeb-3af6-413d-91a4-0e2c161d341c",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>Time of Tweet</th>\n",
              "      <th>Age of User</th>\n",
              "      <th>Country</th>\n",
              "      <th>Population -2020</th>\n",
              "      <th>Land Area (Km²)</th>\n",
              "      <th>Density (P/Km²)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cb774db0d1</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>neutral</td>\n",
              "      <td>morning</td>\n",
              "      <td>0-20</td>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>38928346</td>\n",
              "      <td>652860.0</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>549e992a42</td>\n",
              "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
              "      <td>Sooo SAD</td>\n",
              "      <td>negative</td>\n",
              "      <td>noon</td>\n",
              "      <td>21-30</td>\n",
              "      <td>Albania</td>\n",
              "      <td>2877797</td>\n",
              "      <td>27400.0</td>\n",
              "      <td>105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>088c60f138</td>\n",
              "      <td>my boss is bullying me...</td>\n",
              "      <td>bullying me</td>\n",
              "      <td>negative</td>\n",
              "      <td>night</td>\n",
              "      <td>31-45</td>\n",
              "      <td>Algeria</td>\n",
              "      <td>43851044</td>\n",
              "      <td>2381740.0</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9642c003ef</td>\n",
              "      <td>what interview! leave me alone</td>\n",
              "      <td>leave me alone</td>\n",
              "      <td>negative</td>\n",
              "      <td>morning</td>\n",
              "      <td>46-60</td>\n",
              "      <td>Andorra</td>\n",
              "      <td>77265</td>\n",
              "      <td>470.0</td>\n",
              "      <td>164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>358bd9e861</td>\n",
              "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
              "      <td>Sons of ****,</td>\n",
              "      <td>negative</td>\n",
              "      <td>noon</td>\n",
              "      <td>60-70</td>\n",
              "      <td>Angola</td>\n",
              "      <td>32866272</td>\n",
              "      <td>1246700.0</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       textID                                               text  \\\n",
              "0  cb774db0d1                I`d have responded, if I were going   \n",
              "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
              "2  088c60f138                          my boss is bullying me...   \n",
              "3  9642c003ef                     what interview! leave me alone   \n",
              "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
              "\n",
              "                         selected_text sentiment Time of Tweet Age of User  \\\n",
              "0  I`d have responded, if I were going   neutral       morning        0-20   \n",
              "1                             Sooo SAD  negative          noon       21-30   \n",
              "2                          bullying me  negative         night       31-45   \n",
              "3                       leave me alone  negative       morning       46-60   \n",
              "4                        Sons of ****,  negative          noon       60-70   \n",
              "\n",
              "       Country  Population -2020  Land Area (Km²)  Density (P/Km²)  \n",
              "0  Afghanistan          38928346         652860.0               60  \n",
              "1      Albania           2877797          27400.0              105  \n",
              "2      Algeria          43851044        2381740.0               18  \n",
              "3      Andorra             77265            470.0              164  \n",
              "4       Angola          32866272        1246700.0               26  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLHgZmS_KaeW"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-09-12T08:22:40.282164Z",
          "iopub.status.busy": "2024-09-12T08:22:40.281383Z",
          "iopub.status.idle": "2024-09-12T08:22:40.337247Z",
          "shell.execute_reply": "2024-09-12T08:22:40.335825Z",
          "shell.execute_reply.started": "2024-09-12T08:22:40.282119Z"
        },
        "id": "lQTMOPxnWhs7",
        "outputId": "5595146d-2358-4289-f2bc-c087a7e39b76",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 27481 entries, 0 to 27480\n",
            "Data columns (total 10 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   textID            27481 non-null  object \n",
            " 1   text              27480 non-null  object \n",
            " 2   selected_text     27480 non-null  object \n",
            " 3   sentiment         27481 non-null  object \n",
            " 4   Time of Tweet     27481 non-null  object \n",
            " 5   Age of User       27481 non-null  object \n",
            " 6   Country           27481 non-null  object \n",
            " 7   Population -2020  27481 non-null  int64  \n",
            " 8   Land Area (Km²)   27481 non-null  float64\n",
            " 9   Density (P/Km²)   27481 non-null  int64  \n",
            "dtypes: float64(1), int64(2), object(7)\n",
            "memory usage: 2.1+ MB\n"
          ]
        }
      ],
      "source": [
        "train_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-09-12T08:22:40.340796Z",
          "iopub.status.busy": "2024-09-12T08:22:40.340393Z",
          "iopub.status.idle": "2024-09-12T08:22:40.358799Z",
          "shell.execute_reply": "2024-09-12T08:22:40.357343Z",
          "shell.execute_reply.started": "2024-09-12T08:22:40.340755Z"
        },
        "id": "GmpjaoKBWluP",
        "outputId": "2143527c-74cd-4a50-f732-82d04cd4fdb2",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4815 entries, 0 to 4814\n",
            "Data columns (total 9 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   textID            3534 non-null   object \n",
            " 1   text              3534 non-null   object \n",
            " 2   sentiment         3534 non-null   object \n",
            " 3   Time of Tweet     3534 non-null   object \n",
            " 4   Age of User       3534 non-null   object \n",
            " 5   Country           3534 non-null   object \n",
            " 6   Population -2020  3534 non-null   float64\n",
            " 7   Land Area (Km²)   3534 non-null   float64\n",
            " 8   Density (P/Km²)   3534 non-null   float64\n",
            "dtypes: float64(3), object(6)\n",
            "memory usage: 338.7+ KB\n"
          ]
        }
      ],
      "source": [
        "test_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U95kjy7kKgIC"
      },
      "source": [
        "#### Handling null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "execution": {
          "iopub.execute_input": "2024-09-12T08:22:40.360843Z",
          "iopub.status.busy": "2024-09-12T08:22:40.360383Z",
          "iopub.status.idle": "2024-09-12T08:22:40.402343Z",
          "shell.execute_reply": "2024-09-12T08:22:40.401081Z",
          "shell.execute_reply.started": "2024-09-12T08:22:40.36079Z"
        },
        "id": "RaWIiyJWKeUd",
        "outputId": "6c61868f-1cf3-4f01-b690-d9f168cb98cc",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "textID              0\n",
              "text                1\n",
              "selected_text       1\n",
              "sentiment           0\n",
              "Time of Tweet       0\n",
              "Age of User         0\n",
              "Country             0\n",
              "Population -2020    0\n",
              "Land Area (Km²)     0\n",
              "Density (P/Km²)     0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "execution": {
          "iopub.execute_input": "2024-09-12T08:22:40.404817Z",
          "iopub.status.busy": "2024-09-12T08:22:40.404388Z",
          "iopub.status.idle": "2024-09-12T08:22:40.465463Z",
          "shell.execute_reply": "2024-09-12T08:22:40.463998Z",
          "shell.execute_reply.started": "2024-09-12T08:22:40.404776Z"
        },
        "id": "_5ZDZQ73Kydz",
        "outputId": "a31675d6-563d-4ac8-be41-3b6fd77cfa95",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "textID              0\n",
              "text                0\n",
              "selected_text       0\n",
              "sentiment           0\n",
              "Time of Tweet       0\n",
              "Age of User         0\n",
              "Country             0\n",
              "Population -2020    0\n",
              "Land Area (Km²)     0\n",
              "Density (P/Km²)     0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df = train_df.dropna()\n",
        "train_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "execution": {
          "iopub.execute_input": "2024-09-12T08:22:40.467344Z",
          "iopub.status.busy": "2024-09-12T08:22:40.466939Z",
          "iopub.status.idle": "2024-09-12T08:22:40.481329Z",
          "shell.execute_reply": "2024-09-12T08:22:40.479919Z",
          "shell.execute_reply.started": "2024-09-12T08:22:40.4673Z"
        },
        "id": "x5nimN6_WGRR",
        "outputId": "3eee7f0a-83e3-492f-bcf0-f3edd602ab4c",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "textID              1281\n",
              "text                1281\n",
              "sentiment           1281\n",
              "Time of Tweet       1281\n",
              "Age of User         1281\n",
              "Country             1281\n",
              "Population -2020    1281\n",
              "Land Area (Km²)     1281\n",
              "Density (P/Km²)     1281\n",
              "dtype: int64"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "execution": {
          "iopub.execute_input": "2024-09-12T08:22:40.487599Z",
          "iopub.status.busy": "2024-09-12T08:22:40.486547Z",
          "iopub.status.idle": "2024-09-12T08:22:40.507416Z",
          "shell.execute_reply": "2024-09-12T08:22:40.506177Z",
          "shell.execute_reply.started": "2024-09-12T08:22:40.487539Z"
        },
        "id": "A3hqNN4GWP9J",
        "outputId": "d37c315f-eccb-42f3-9563-941610c401d5",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "textID              0\n",
              "text                0\n",
              "sentiment           0\n",
              "Time of Tweet       0\n",
              "Age of User         0\n",
              "Country             0\n",
              "Population -2020    0\n",
              "Land Area (Km²)     0\n",
              "Density (P/Km²)     0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df = test_df.dropna()\n",
        "test_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOgrqQewLCdw"
      },
      "source": [
        "#### Removing stopwords & lowercase all text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-09-12T08:22:40.509448Z",
          "iopub.status.busy": "2024-09-12T08:22:40.508972Z",
          "iopub.status.idle": "2024-09-12T08:22:40.598748Z",
          "shell.execute_reply": "2024-09-12T08:22:40.597069Z",
          "shell.execute_reply.started": "2024-09-12T08:22:40.509394Z"
        },
        "id": "Qpsxd4BWNHJO",
        "outputId": "f1abd31d-5bd5-4b9f-de18-4cb44ce81a10",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-12T08:22:40.601099Z",
          "iopub.status.busy": "2024-09-12T08:22:40.600609Z",
          "iopub.status.idle": "2024-09-12T08:22:40.609951Z",
          "shell.execute_reply": "2024-09-12T08:22:40.608024Z",
          "shell.execute_reply.started": "2024-09-12T08:22:40.601044Z"
        },
        "id": "GyCFZy2FOZ0u",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Text preprocessing function that removes stopwords and convert text to lowercase\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "execution": {
          "iopub.execute_input": "2024-09-12T08:22:40.611901Z",
          "iopub.status.busy": "2024-09-12T08:22:40.611511Z",
          "iopub.status.idle": "2024-09-12T08:22:45.335514Z",
          "shell.execute_reply": "2024-09-12T08:22:45.334329Z",
          "shell.execute_reply.started": "2024-09-12T08:22:40.611861Z"
        },
        "id": "6Rd5y7pkOyXz",
        "outputId": "e9b5306f-5c8d-46c9-fa42-c4856f39cf98",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>Time of Tweet</th>\n",
              "      <th>Age of User</th>\n",
              "      <th>Country</th>\n",
              "      <th>Population -2020</th>\n",
              "      <th>Land Area (Km²)</th>\n",
              "      <th>Density (P/Km²)</th>\n",
              "      <th>processed_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cb774db0d1</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>neutral</td>\n",
              "      <td>morning</td>\n",
              "      <td>0-20</td>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>38928346</td>\n",
              "      <td>652860.0</td>\n",
              "      <td>60</td>\n",
              "      <td>i`d responded, going</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>549e992a42</td>\n",
              "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
              "      <td>Sooo SAD</td>\n",
              "      <td>negative</td>\n",
              "      <td>noon</td>\n",
              "      <td>21-30</td>\n",
              "      <td>Albania</td>\n",
              "      <td>2877797</td>\n",
              "      <td>27400.0</td>\n",
              "      <td>105</td>\n",
              "      <td>sooo sad miss san diego!!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>088c60f138</td>\n",
              "      <td>my boss is bullying me...</td>\n",
              "      <td>bullying me</td>\n",
              "      <td>negative</td>\n",
              "      <td>night</td>\n",
              "      <td>31-45</td>\n",
              "      <td>Algeria</td>\n",
              "      <td>43851044</td>\n",
              "      <td>2381740.0</td>\n",
              "      <td>18</td>\n",
              "      <td>boss bullying me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9642c003ef</td>\n",
              "      <td>what interview! leave me alone</td>\n",
              "      <td>leave me alone</td>\n",
              "      <td>negative</td>\n",
              "      <td>morning</td>\n",
              "      <td>46-60</td>\n",
              "      <td>Andorra</td>\n",
              "      <td>77265</td>\n",
              "      <td>470.0</td>\n",
              "      <td>164</td>\n",
              "      <td>interview! leave alone</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>358bd9e861</td>\n",
              "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
              "      <td>Sons of ****,</td>\n",
              "      <td>negative</td>\n",
              "      <td>noon</td>\n",
              "      <td>60-70</td>\n",
              "      <td>Angola</td>\n",
              "      <td>32866272</td>\n",
              "      <td>1246700.0</td>\n",
              "      <td>26</td>\n",
              "      <td>sons ****, couldn`t put releases already bought</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       textID                                               text  \\\n",
              "0  cb774db0d1                I`d have responded, if I were going   \n",
              "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
              "2  088c60f138                          my boss is bullying me...   \n",
              "3  9642c003ef                     what interview! leave me alone   \n",
              "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
              "\n",
              "                         selected_text sentiment Time of Tweet Age of User  \\\n",
              "0  I`d have responded, if I were going   neutral       morning        0-20   \n",
              "1                             Sooo SAD  negative          noon       21-30   \n",
              "2                          bullying me  negative         night       31-45   \n",
              "3                       leave me alone  negative       morning       46-60   \n",
              "4                        Sons of ****,  negative          noon       60-70   \n",
              "\n",
              "       Country  Population -2020  Land Area (Km²)  Density (P/Km²)  \\\n",
              "0  Afghanistan          38928346         652860.0               60   \n",
              "1      Albania           2877797          27400.0              105   \n",
              "2      Algeria          43851044        2381740.0               18   \n",
              "3      Andorra             77265            470.0              164   \n",
              "4       Angola          32866272        1246700.0               26   \n",
              "\n",
              "                                    processed_text  \n",
              "0                             i`d responded, going  \n",
              "1                       sooo sad miss san diego!!!  \n",
              "2                              boss bullying me...  \n",
              "3                           interview! leave alone  \n",
              "4  sons ****, couldn`t put releases already bought  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Apply preprocessing on train dataset\n",
        "train_df['processed_text'] = train_df['text'].apply(preprocess_text)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "execution": {
          "iopub.execute_input": "2024-09-12T08:22:45.337762Z",
          "iopub.status.busy": "2024-09-12T08:22:45.337232Z",
          "iopub.status.idle": "2024-09-12T08:22:45.959644Z",
          "shell.execute_reply": "2024-09-12T08:22:45.958512Z",
          "shell.execute_reply.started": "2024-09-12T08:22:45.337709Z"
        },
        "id": "2Vpcp2FVXXVQ",
        "outputId": "c6770cf5-f897-410a-b6ae-7df5bd9181a2",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>Time of Tweet</th>\n",
              "      <th>Age of User</th>\n",
              "      <th>Country</th>\n",
              "      <th>Population -2020</th>\n",
              "      <th>Land Area (Km²)</th>\n",
              "      <th>Density (P/Km²)</th>\n",
              "      <th>processed_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>f87dea47db</td>\n",
              "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
              "      <td>neutral</td>\n",
              "      <td>morning</td>\n",
              "      <td>0-20</td>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>38928346.0</td>\n",
              "      <td>652860.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>last session day http://twitpic.com/67ezh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>96d74cb729</td>\n",
              "      <td>Shanghai is also really exciting (precisely -...</td>\n",
              "      <td>positive</td>\n",
              "      <td>noon</td>\n",
              "      <td>21-30</td>\n",
              "      <td>Albania</td>\n",
              "      <td>2877797.0</td>\n",
              "      <td>27400.0</td>\n",
              "      <td>105.0</td>\n",
              "      <td>shanghai also really exciting (precisely -- sk...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>eee518ae67</td>\n",
              "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
              "      <td>negative</td>\n",
              "      <td>night</td>\n",
              "      <td>31-45</td>\n",
              "      <td>Algeria</td>\n",
              "      <td>43851044.0</td>\n",
              "      <td>2381740.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>recession hit veronique branquinho, quit compa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>01082688c6</td>\n",
              "      <td>happy bday!</td>\n",
              "      <td>positive</td>\n",
              "      <td>morning</td>\n",
              "      <td>46-60</td>\n",
              "      <td>Andorra</td>\n",
              "      <td>77265.0</td>\n",
              "      <td>470.0</td>\n",
              "      <td>164.0</td>\n",
              "      <td>happy bday!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33987a8ee5</td>\n",
              "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
              "      <td>positive</td>\n",
              "      <td>noon</td>\n",
              "      <td>60-70</td>\n",
              "      <td>Angola</td>\n",
              "      <td>32866272.0</td>\n",
              "      <td>1246700.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>http://twitpic.com/4w75p - like it!!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       textID                                               text sentiment  \\\n",
              "0  f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral   \n",
              "1  96d74cb729   Shanghai is also really exciting (precisely -...  positive   \n",
              "2  eee518ae67  Recession hit Veronique Branquinho, she has to...  negative   \n",
              "3  01082688c6                                        happy bday!  positive   \n",
              "4  33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive   \n",
              "\n",
              "  Time of Tweet Age of User      Country  Population -2020  Land Area (Km²)  \\\n",
              "0       morning        0-20  Afghanistan        38928346.0         652860.0   \n",
              "1          noon       21-30      Albania         2877797.0          27400.0   \n",
              "2         night       31-45      Algeria        43851044.0        2381740.0   \n",
              "3       morning       46-60      Andorra           77265.0            470.0   \n",
              "4          noon       60-70       Angola        32866272.0        1246700.0   \n",
              "\n",
              "   Density (P/Km²)                                     processed_text  \n",
              "0             60.0          last session day http://twitpic.com/67ezh  \n",
              "1            105.0  shanghai also really exciting (precisely -- sk...  \n",
              "2             18.0  recession hit veronique branquinho, quit compa...  \n",
              "3            164.0                                        happy bday!  \n",
              "4             26.0               http://twitpic.com/4w75p - like it!!  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Apply preprocessing on test dataset\n",
        "test_df['processed_text'] = test_df['text'].apply(preprocess_text)\n",
        "test_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9FZF-S6RwT0"
      },
      "source": [
        "## Check Imbalancing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "execution": {
          "iopub.execute_input": "2024-09-12T08:27:19.351826Z",
          "iopub.status.busy": "2024-09-12T08:27:19.351335Z",
          "iopub.status.idle": "2024-09-12T08:27:19.610508Z",
          "shell.execute_reply": "2024-09-12T08:27:19.609354Z",
          "shell.execute_reply.started": "2024-09-12T08:27:19.351775Z"
        },
        "id": "uRP-7dXHUcgb",
        "outputId": "c55e8422-deb7-49b4-b029-1d1d3a550c50",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Distribution of sentiment labels\n",
        "sns.countplot(x='sentiment', data=train_df)\n",
        "plt.title(\"Distribution of Sentiment Labels\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ppFEsZZXIW75"
      },
      "outputs": [],
      "source": [
        "# 1. Transform sentiment into 3 classes\n",
        "# Example mapping: positive -> 2, neutral -> 1, negative -> 0\n",
        "sentiment_mapping = {\"positive\": 2, \"neutral\": 1, \"negative\": 0}\n",
        "train_df[\"sentiment_class\"] = train_df[\"sentiment\"].map(sentiment_mapping)\n",
        "test_df[\"sentiment_class\"] = test_df[\"sentiment\"].map(sentiment_mapping)\n",
        "\n",
        "# 2. Extract all the values from the 'processed_text' column into a list\n",
        "trainval_x = train_df[\"processed_text\"].tolist()\n",
        "trainval_y = train_df[\"sentiment_class\"].tolist()\n",
        "\n",
        "train_x, val_x, train_y, val_y = train_test_split(trainval_x, trainval_y, test_size=0.25, random_state=42)\n",
        "\n",
        "test_x = test_df[\"processed_text\"].tolist()\n",
        "test_y = test_df[\"sentiment_class\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGje1w01rjlw",
        "outputId": "862fa31c-ba1a-4f4d-ce50-676b87f2b2a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27480 6870 3534\n"
          ]
        }
      ],
      "source": [
        "print(len(trainval_x),len(val_x),len(test_x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "meidY0NVsChh"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "PRETRAINED_MODEL = \"bert-base-uncased\"\n",
        "MAX_LENGTH = 128\n",
        "BATCH_SIZE = 64\n",
        "NUM_CLASSES = 3\n",
        "LEARNING_RATE = 2e-5\n",
        "EPOCHS = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Custom Dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            texts (list): List of text samples.\n",
        "            labels (list): List of sentiment labels (e.g., 0, 1).\n",
        "            tokenizer (transformers.BertTokenizer): Tokenizer for BERT.\n",
        "            max_length (int): Maximum length for tokenized sequences.\n",
        "        \"\"\"\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Tokenize and encode the text\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"label\": torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Extract embeddings for all data\n",
        "def extract_embeddings(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Extracts embeddings for all data using a pre-trained BERT model.\n",
        "\n",
        "    Args:\n",
        "        model (transformers.BertModel): Pre-trained BERT model.\n",
        "        dataloader (DataLoader): DataLoader for the dataset.\n",
        "        device (torch.device): Device to run the model on (CPU or GPU).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: A matrix of size (number_of_samples, embedding_size).\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    embeddings = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            # Forward pass through BERT\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            pooled_output = outputs.pooler_output  # CLS token representation\n",
        "\n",
        "            # Append embeddings to the list\n",
        "            embeddings.append(pooled_output.cpu())\n",
        "\n",
        "    # Combine all embeddings into a single matrix\n",
        "    return torch.cat(embeddings, dim=0)\n",
        "\n",
        "# Initialize tokenizer, dataset, and dataloader\n",
        "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
        "traindataset = TextDataset(train_x, train_y, tokenizer, MAX_LENGTH)\n",
        "trainloader = DataLoader(traindataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "valdataset = TextDataset(val_x, val_y, tokenizer, MAX_LENGTH)\n",
        "valloader = DataLoader(valdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "bert = BertModel.from_pretrained(PRETRAINED_MODEL).to(device)\n",
        "# Extract embeddings\n",
        "train_embeddings = extract_embeddings(bert, trainloader, device)\n",
        "train_embeddings =train_embeddings.cpu()\n",
        "\n",
        "val_embeddings = extract_embeddings(bert, valloader, device)\n",
        "val_embeddings =val_embeddings.cpu()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWzk4bUcvX03",
        "outputId": "a905e572-cd4f-4db6-9117-16f9e062d890"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([20610, 768])\n",
            "torch.Size([6870, 768])\n"
          ]
        }
      ],
      "source": [
        "print(train_embeddings.size())\n",
        "print(val_embeddings.size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "lHR_N9OiwgYL"
      },
      "outputs": [],
      "source": [
        "criterion= nn.CrossEntropyLoss()\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "# Custom Dataset\n",
        "class EmbeddingDataset(Dataset):\n",
        "    def __init__(self, embeddings, labels):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            texts (list): List of text samples.\n",
        "            labels (list): List of sentiment labels (e.g., 0, 1).\n",
        "            tokenizer (transformers.BertTokenizer): Tokenizer for BERT.\n",
        "            max_length (int): Maximum length for tokenized sequences.\n",
        "        \"\"\"\n",
        "        self.embeddings = embeddings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.embeddings)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Tokenize and encode the text\n",
        "        embeddings = self.embeddings[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": embeddings.squeeze(0),\n",
        "            \"label\": torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Model Definition\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, hidden_size, num_classes):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "\n",
        "        # Pass through the fully connected layer\n",
        "        logits = self.fc(input_ids)\n",
        "        return logits\n",
        "\n",
        "# training script\n",
        "\n",
        "def train( model, train_loader, optimizer, epoch,log_interval=50):\n",
        "    model.train()\n",
        "    loss_cpu=0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, data in enumerate(train_loader, 0):\n",
        "\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, target = data['input_ids'],data['label']\n",
        "        inputs, target = inputs.cuda(), target.cuda()\n",
        "        inputs =inputs.detach()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        loss_cpu+= loss.item()\n",
        "        total += target.size(0)\n",
        "        correct += predicted.eq(target.data).cpu().sum()\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%'\n",
        "                %(epoch, EPOCHS, batch_idx+1,\n",
        "                    (len(train_loader)//BATCH_SIZE)+1, loss.item(), 100.*correct/total))\n",
        "            #n_iter=epoch * len(train_loader) + batch_idx\n",
        "\n",
        "    return loss_cpu/len(train_loader)\n",
        "\n",
        "# testing script\n",
        "def test( model, test_loader,epoch):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    test_loss_MSE =0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(test_loader, 0):\n",
        "            inputs, target = data['input_ids'],data['label']\n",
        "            inputs, target = inputs.cuda(), target.cuda()\n",
        "            outputs  = model(inputs)\n",
        "            loss = criterion(outputs,target)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target.data).cpu().sum()\n",
        "            test_loss_MSE+= loss.item()\n",
        "\n",
        "    test_loss_MSE = test_loss_MSE/ len(test_loader)\n",
        "    print(\"\\n| Validation Epoch #%d\\t\\t\\tLoss: %.4f Acc@1: %.2f%%\" %(epoch, loss.item(), 100.*correct/total))\n",
        "    return test_loss_MSE, 100.*correct/total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0X6VeclzYro",
        "outputId": "1d64bacd-dc0d-42b0-a915-387380137513"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Let us Train.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "3/ training model 1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch [  0/ 50] Iter[  1/  6]\t\tLoss: 1.1336 Acc@1: 31.250%\n",
            "| Epoch [  0/ 50] Iter[ 51/  6]\t\tLoss: 4.4515 Acc@1: 36.275%\n",
            "| Epoch [  0/ 50] Iter[101/  6]\t\tLoss: 4.9395 Acc@1: 33.694%\n",
            "| Epoch [  0/ 50] Iter[151/  6]\t\tLoss: 17.0216 Acc@1: 35.989%\n",
            "| Epoch [  0/ 50] Iter[201/  6]\t\tLoss: 14.8274 Acc@1: 35.137%\n",
            "| Epoch [  0/ 50] Iter[251/  6]\t\tLoss: 13.6224 Acc@1: 36.149%\n",
            "| Epoch [  0/ 50] Iter[301/  6]\t\tLoss: 36.4469 Acc@1: 35.060%\n",
            "\n",
            "| Validation Epoch #0\t\t\tLoss: 34.8573 Acc@1: 28.54%\n",
            "lr = 0.001\n",
            "Best RMSE is of : 32.824568112691246 for epoch : 0 ERROR TEST =  (32.824568112691246, tensor(28.5444))\n",
            "| Epoch [  1/ 50] Iter[  1/  6]\t\tLoss: 32.9517 Acc@1: 29.688%\n",
            "| Epoch [  1/ 50] Iter[ 51/  6]\t\tLoss: 25.4992 Acc@1: 29.075%\n",
            "| Epoch [  1/ 50] Iter[101/  6]\t\tLoss: 27.9848 Acc@1: 31.405%\n",
            "| Epoch [  1/ 50] Iter[151/  6]\t\tLoss: 65.7985 Acc@1: 34.178%\n",
            "| Epoch [  1/ 50] Iter[201/  6]\t\tLoss: 49.8069 Acc@1: 35.673%\n",
            "| Epoch [  1/ 50] Iter[251/  6]\t\tLoss: 27.5411 Acc@1: 36.398%\n",
            "| Epoch [  1/ 50] Iter[301/  6]\t\tLoss: 41.0543 Acc@1: 35.803%\n",
            "\n",
            "| Validation Epoch #1\t\t\tLoss: 24.6880 Acc@1: 31.00%\n",
            "lr = 0.001\n",
            "Best RMSE is of : 30.859816586529767 for epoch : 1 ERROR TEST =  (30.859816586529767, tensor(31.0044))\n",
            "| Epoch [  2/ 50] Iter[  1/  6]\t\tLoss: 36.0205 Acc@1: 20.312%\n",
            "| Epoch [  2/ 50] Iter[ 51/  6]\t\tLoss: 58.4477 Acc@1: 29.412%\n",
            "| Epoch [  2/ 50] Iter[101/  6]\t\tLoss: 60.9093 Acc@1: 29.022%\n",
            "| Epoch [  2/ 50] Iter[151/  6]\t\tLoss: 40.0840 Acc@1: 28.880%\n",
            "| Epoch [  2/ 50] Iter[201/  6]\t\tLoss: 35.2951 Acc@1: 30.597%\n",
            "| Epoch [  2/ 50] Iter[251/  6]\t\tLoss: 51.9388 Acc@1: 32.676%\n",
            "| Epoch [  2/ 50] Iter[301/  6]\t\tLoss: 38.7873 Acc@1: 33.861%\n",
            "\n",
            "| Validation Epoch #2\t\t\tLoss: 37.3084 Acc@1: 37.80%\n",
            "lr = 0.001\n",
            "| Epoch [  3/ 50] Iter[  1/  6]\t\tLoss: 48.5736 Acc@1: 42.188%\n",
            "| Epoch [  3/ 50] Iter[ 51/  6]\t\tLoss: 58.8996 Acc@1: 32.292%\n",
            "| Epoch [  3/ 50] Iter[101/  6]\t\tLoss: 62.3096 Acc@1: 31.652%\n",
            "| Epoch [  3/ 50] Iter[151/  6]\t\tLoss: 52.6432 Acc@1: 32.057%\n",
            "| Epoch [  3/ 50] Iter[201/  6]\t\tLoss: 66.3074 Acc@1: 34.087%\n",
            "| Epoch [  3/ 50] Iter[251/  6]\t\tLoss: 54.4865 Acc@1: 35.060%\n",
            "| Epoch [  3/ 50] Iter[301/  6]\t\tLoss: 39.0500 Acc@1: 35.958%\n",
            "\n",
            "| Validation Epoch #3\t\t\tLoss: 30.7499 Acc@1: 40.55%\n",
            "lr = 0.001\n",
            "Best RMSE is of : 26.736634766613996 for epoch : 3 ERROR TEST =  (26.736634766613996, tensor(40.5531))\n",
            "| Epoch [  4/ 50] Iter[  1/  6]\t\tLoss: 23.4701 Acc@1: 31.250%\n",
            "| Epoch [  4/ 50] Iter[ 51/  6]\t\tLoss: 46.9262 Acc@1: 32.966%\n",
            "| Epoch [  4/ 50] Iter[101/  6]\t\tLoss: 67.7499 Acc@1: 30.538%\n",
            "| Epoch [  4/ 50] Iter[151/  6]\t\tLoss: 73.6648 Acc@1: 29.512%\n",
            "| Epoch [  4/ 50] Iter[201/  6]\t\tLoss: 57.0599 Acc@1: 29.221%\n",
            "| Epoch [  4/ 50] Iter[251/  6]\t\tLoss: 55.8871 Acc@1: 29.806%\n",
            "| Epoch [  4/ 50] Iter[301/  6]\t\tLoss: 66.2275 Acc@1: 29.921%\n",
            "\n",
            "| Validation Epoch #4\t\t\tLoss: 54.5692 Acc@1: 30.98%\n",
            "lr = 0.001\n",
            "| Epoch [  5/ 50] Iter[  1/  6]\t\tLoss: 61.3664 Acc@1: 25.000%\n",
            "| Epoch [  5/ 50] Iter[ 51/  6]\t\tLoss: 36.0419 Acc@1: 30.913%\n",
            "| Epoch [  5/ 50] Iter[101/  6]\t\tLoss: 46.3099 Acc@1: 36.139%\n",
            "| Epoch [  5/ 50] Iter[151/  6]\t\tLoss: 75.6213 Acc@1: 37.810%\n",
            "| Epoch [  5/ 50] Iter[201/  6]\t\tLoss: 95.7561 Acc@1: 38.091%\n",
            "| Epoch [  5/ 50] Iter[251/  6]\t\tLoss: 96.0166 Acc@1: 38.546%\n",
            "| Epoch [  5/ 50] Iter[301/  6]\t\tLoss: 80.7375 Acc@1: 39.062%\n",
            "\n",
            "| Validation Epoch #5\t\t\tLoss: 60.1148 Acc@1: 40.57%\n",
            "lr = 0.001\n",
            "| Epoch [  6/ 50] Iter[  1/  6]\t\tLoss: 69.9580 Acc@1: 35.938%\n",
            "| Epoch [  6/ 50] Iter[ 51/  6]\t\tLoss: 32.4351 Acc@1: 40.717%\n",
            "| Epoch [  6/ 50] Iter[101/  6]\t\tLoss: 15.6154 Acc@1: 41.878%\n",
            "| Epoch [  6/ 50] Iter[151/  6]\t\tLoss: 37.9968 Acc@1: 41.029%\n",
            "| Epoch [  6/ 50] Iter[201/  6]\t\tLoss: 64.4290 Acc@1: 40.159%\n",
            "| Epoch [  6/ 50] Iter[251/  6]\t\tLoss: 71.9718 Acc@1: 38.558%\n",
            "| Epoch [  6/ 50] Iter[301/  6]\t\tLoss: 78.6833 Acc@1: 37.542%\n",
            "\n",
            "| Validation Epoch #6\t\t\tLoss: 75.8795 Acc@1: 40.76%\n",
            "lr = 0.001\n",
            "| Epoch [  7/ 50] Iter[  1/  6]\t\tLoss: 100.0774 Acc@1: 37.500%\n",
            "| Epoch [  7/ 50] Iter[ 51/  6]\t\tLoss: 86.6428 Acc@1: 37.745%\n",
            "| Epoch [  7/ 50] Iter[101/  6]\t\tLoss: 92.3211 Acc@1: 35.241%\n",
            "| Epoch [  7/ 50] Iter[151/  6]\t\tLoss: 75.6014 Acc@1: 35.141%\n",
            "| Epoch [  7/ 50] Iter[201/  6]\t\tLoss: 48.0735 Acc@1: 35.766%\n",
            "| Epoch [  7/ 50] Iter[251/  6]\t\tLoss: 63.8095 Acc@1: 34.655%\n",
            "| Epoch [  7/ 50] Iter[301/  6]\t\tLoss: 29.0584 Acc@1: 33.674%\n",
            "\n",
            "| Validation Epoch #7\t\t\tLoss: 13.7056 Acc@1: 40.38%\n",
            "lr = 0.001\n",
            "Best RMSE is of : 13.954051468107435 for epoch : 7 ERROR TEST =  (13.954051468107435, tensor(40.3785))\n",
            "| Epoch [  8/ 50] Iter[  1/  6]\t\tLoss: 15.4585 Acc@1: 39.062%\n",
            "| Epoch [  8/ 50] Iter[ 51/  6]\t\tLoss: 26.5867 Acc@1: 44.240%\n",
            "| Epoch [  8/ 50] Iter[101/  6]\t\tLoss: 52.1847 Acc@1: 42.358%\n",
            "| Epoch [  8/ 50] Iter[151/  6]\t\tLoss: 68.5261 Acc@1: 41.784%\n",
            "| Epoch [  8/ 50] Iter[201/  6]\t\tLoss: 80.3695 Acc@1: 41.433%\n",
            "| Epoch [  8/ 50] Iter[251/  6]\t\tLoss: 66.5822 Acc@1: 41.440%\n",
            "| Epoch [  8/ 50] Iter[301/  6]\t\tLoss: 63.3219 Acc@1: 41.653%\n",
            "\n",
            "| Validation Epoch #8\t\t\tLoss: 84.2591 Acc@1: 40.63%\n",
            "lr = 0.001\n",
            "| Epoch [  9/ 50] Iter[  1/  6]\t\tLoss: 71.1363 Acc@1: 43.750%\n",
            "| Epoch [  9/ 50] Iter[ 51/  6]\t\tLoss: 100.6363 Acc@1: 36.703%\n",
            "| Epoch [  9/ 50] Iter[101/  6]\t\tLoss: 111.2958 Acc@1: 35.721%\n",
            "| Epoch [  9/ 50] Iter[151/  6]\t\tLoss: 94.2935 Acc@1: 35.472%\n",
            "| Epoch [  9/ 50] Iter[201/  6]\t\tLoss: 68.0358 Acc@1: 35.409%\n",
            "| Epoch [  9/ 50] Iter[251/  6]\t\tLoss: 39.1507 Acc@1: 37.326%\n",
            "| Epoch [  9/ 50] Iter[301/  6]\t\tLoss: 63.3469 Acc@1: 37.988%\n",
            "\n",
            "| Validation Epoch #9\t\t\tLoss: 67.6675 Acc@1: 40.66%\n",
            "lr = 0.001\n",
            "| Epoch [ 10/ 50] Iter[  1/  6]\t\tLoss: 64.9732 Acc@1: 40.625%\n",
            "| Epoch [ 10/ 50] Iter[ 51/  6]\t\tLoss: 47.4480 Acc@1: 41.360%\n",
            "| Epoch [ 10/ 50] Iter[101/  6]\t\tLoss: 32.8610 Acc@1: 41.785%\n",
            "| Epoch [ 10/ 50] Iter[151/  6]\t\tLoss: 37.7787 Acc@1: 41.029%\n",
            "| Epoch [ 10/ 50] Iter[201/  6]\t\tLoss: 81.1424 Acc@1: 38.557%\n",
            "| Epoch [ 10/ 50] Iter[251/  6]\t\tLoss: 96.5551 Acc@1: 36.672%\n",
            "| Epoch [ 10/ 50] Iter[301/  6]\t\tLoss: 82.8405 Acc@1: 35.154%\n",
            "\n",
            "| Validation Epoch #10\t\t\tLoss: 102.7056 Acc@1: 28.53%\n",
            "lr = 0.001\n",
            "| Epoch [ 11/ 50] Iter[  1/  6]\t\tLoss: 99.6065 Acc@1: 23.438%\n",
            "| Epoch [ 11/ 50] Iter[ 51/  6]\t\tLoss: 71.7040 Acc@1: 28.431%\n",
            "| Epoch [ 11/ 50] Iter[101/  6]\t\tLoss: 38.9260 Acc@1: 29.223%\n",
            "| Epoch [ 11/ 50] Iter[151/  6]\t\tLoss: 29.6494 Acc@1: 32.067%\n",
            "| Epoch [ 11/ 50] Iter[201/  6]\t\tLoss: 43.7897 Acc@1: 32.502%\n",
            "| Epoch [ 11/ 50] Iter[251/  6]\t\tLoss: 51.2105 Acc@1: 32.644%\n",
            "| Epoch [ 11/ 50] Iter[301/  6]\t\tLoss: 49.8080 Acc@1: 33.456%\n",
            "\n",
            "| Validation Epoch #11\t\t\tLoss: 66.9416 Acc@1: 44.85%\n",
            "lr = 0.001\n",
            "| Epoch [ 12/ 50] Iter[  1/  6]\t\tLoss: 49.7419 Acc@1: 50.000%\n",
            "| Epoch [ 12/ 50] Iter[ 51/  6]\t\tLoss: 70.9564 Acc@1: 44.516%\n",
            "| Epoch [ 12/ 50] Iter[101/  6]\t\tLoss: 90.2788 Acc@1: 43.533%\n",
            "| Epoch [ 12/ 50] Iter[151/  6]\t\tLoss: 118.1063 Acc@1: 42.229%\n",
            "| Epoch [ 12/ 50] Iter[201/  6]\t\tLoss: 122.3585 Acc@1: 41.908%\n",
            "| Epoch [ 12/ 50] Iter[251/  6]\t\tLoss: 91.1113 Acc@1: 41.752%\n",
            "| Epoch [ 12/ 50] Iter[301/  6]\t\tLoss: 83.0551 Acc@1: 42.021%\n",
            "\n",
            "| Validation Epoch #12\t\t\tLoss: 56.4498 Acc@1: 45.15%\n",
            "lr = 0.001\n",
            "| Epoch [ 13/ 50] Iter[  1/  6]\t\tLoss: 78.8391 Acc@1: 43.750%\n",
            "| Epoch [ 13/ 50] Iter[ 51/  6]\t\tLoss: 92.3799 Acc@1: 40.043%\n",
            "| Epoch [ 13/ 50] Iter[101/  6]\t\tLoss: 101.3853 Acc@1: 35.412%\n",
            "| Epoch [ 13/ 50] Iter[151/  6]\t\tLoss: 78.3242 Acc@1: 34.489%\n",
            "| Epoch [ 13/ 50] Iter[201/  6]\t\tLoss: 76.4293 Acc@1: 34.056%\n",
            "| Epoch [ 13/ 50] Iter[251/  6]\t\tLoss: 40.9209 Acc@1: 34.095%\n",
            "| Epoch [ 13/ 50] Iter[301/  6]\t\tLoss: 24.7166 Acc@1: 36.119%\n",
            "\n",
            "| Validation Epoch #13\t\t\tLoss: 17.2195 Acc@1: 46.10%\n",
            "lr = 0.001\n",
            "| Epoch [ 14/ 50] Iter[  1/  6]\t\tLoss: 18.6520 Acc@1: 46.875%\n",
            "| Epoch [ 14/ 50] Iter[ 51/  6]\t\tLoss: 25.1249 Acc@1: 43.505%\n",
            "| Epoch [ 14/ 50] Iter[101/  6]\t\tLoss: 55.5251 Acc@1: 41.460%\n",
            "| Epoch [ 14/ 50] Iter[151/  6]\t\tLoss: 62.9002 Acc@1: 38.969%\n",
            "| Epoch [ 14/ 50] Iter[201/  6]\t\tLoss: 94.5477 Acc@1: 36.482%\n",
            "| Epoch [ 14/ 50] Iter[251/  6]\t\tLoss: 119.9426 Acc@1: 35.010%\n",
            "| Epoch [ 14/ 50] Iter[301/  6]\t\tLoss: 102.7158 Acc@1: 34.557%\n",
            "\n",
            "| Validation Epoch #14\t\t\tLoss: 75.6470 Acc@1: 35.21%\n",
            "lr = 0.001\n",
            "| Epoch [ 15/ 50] Iter[  1/  6]\t\tLoss: 74.5093 Acc@1: 37.500%\n",
            "| Epoch [ 15/ 50] Iter[ 51/  6]\t\tLoss: 75.1033 Acc@1: 39.216%\n",
            "| Epoch [ 15/ 50] Iter[101/  6]\t\tLoss: 99.6912 Acc@1: 40.099%\n",
            "| Epoch [ 15/ 50] Iter[151/  6]\t\tLoss: 85.7577 Acc@1: 40.159%\n",
            "| Epoch [ 15/ 50] Iter[201/  6]\t\tLoss: 105.0145 Acc@1: 40.400%\n",
            "| Epoch [ 15/ 50] Iter[251/  6]\t\tLoss: 86.8111 Acc@1: 40.712%\n",
            "| Epoch [ 15/ 50] Iter[301/  6]\t\tLoss: 46.7860 Acc@1: 40.599%\n",
            "\n",
            "| Validation Epoch #15\t\t\tLoss: 44.0178 Acc@1: 40.82%\n",
            "lr = 0.0001\n",
            "| Epoch [ 16/ 50] Iter[  1/  6]\t\tLoss: 56.5979 Acc@1: 28.125%\n",
            "| Epoch [ 16/ 50] Iter[ 51/  6]\t\tLoss: 35.2379 Acc@1: 41.330%\n",
            "| Epoch [ 16/ 50] Iter[101/  6]\t\tLoss: 26.1048 Acc@1: 41.600%\n",
            "| Epoch [ 16/ 50] Iter[151/  6]\t\tLoss: 23.2696 Acc@1: 41.732%\n",
            "| Epoch [ 16/ 50] Iter[201/  6]\t\tLoss: 24.3349 Acc@1: 41.682%\n",
            "| Epoch [ 16/ 50] Iter[251/  6]\t\tLoss: 18.8349 Acc@1: 42.268%\n",
            "| Epoch [ 16/ 50] Iter[301/  6]\t\tLoss: 21.9879 Acc@1: 42.868%\n",
            "\n",
            "| Validation Epoch #16\t\t\tLoss: 25.2379 Acc@1: 46.91%\n",
            "lr = 0.0001\n",
            "| Epoch [ 17/ 50] Iter[  1/  6]\t\tLoss: 17.8214 Acc@1: 45.312%\n",
            "| Epoch [ 17/ 50] Iter[ 51/  6]\t\tLoss: 19.8236 Acc@1: 47.978%\n",
            "| Epoch [ 17/ 50] Iter[101/  6]\t\tLoss: 12.8301 Acc@1: 47.850%\n",
            "| Epoch [ 17/ 50] Iter[151/  6]\t\tLoss: 15.4035 Acc@1: 48.396%\n",
            "| Epoch [ 17/ 50] Iter[201/  6]\t\tLoss: 10.6454 Acc@1: 48.997%\n",
            "| Epoch [ 17/ 50] Iter[251/  6]\t\tLoss: 16.1470 Acc@1: 49.048%\n",
            "| Epoch [ 17/ 50] Iter[301/  6]\t\tLoss: 15.3464 Acc@1: 49.138%\n",
            "\n",
            "| Validation Epoch #17\t\t\tLoss: 14.8325 Acc@1: 48.59%\n",
            "lr = 0.0001\n",
            "| Epoch [ 18/ 50] Iter[  1/  6]\t\tLoss: 12.2402 Acc@1: 48.438%\n",
            "| Epoch [ 18/ 50] Iter[ 51/  6]\t\tLoss: 16.5351 Acc@1: 49.663%\n",
            "| Epoch [ 18/ 50] Iter[101/  6]\t\tLoss: 17.3789 Acc@1: 49.520%\n",
            "| Epoch [ 18/ 50] Iter[151/  6]\t\tLoss: 18.8334 Acc@1: 49.183%\n",
            "| Epoch [ 18/ 50] Iter[201/  6]\t\tLoss: 24.5140 Acc@1: 48.725%\n",
            "| Epoch [ 18/ 50] Iter[251/  6]\t\tLoss: 25.6625 Acc@1: 48.419%\n",
            "| Epoch [ 18/ 50] Iter[301/  6]\t\tLoss: 20.5926 Acc@1: 47.830%\n",
            "\n",
            "| Validation Epoch #18\t\t\tLoss: 12.9303 Acc@1: 44.76%\n",
            "lr = 0.0001\n",
            "| Epoch [ 19/ 50] Iter[  1/  6]\t\tLoss: 26.1469 Acc@1: 40.625%\n",
            "| Epoch [ 19/ 50] Iter[ 51/  6]\t\tLoss: 17.0372 Acc@1: 44.638%\n",
            "| Epoch [ 19/ 50] Iter[101/  6]\t\tLoss: 25.3351 Acc@1: 43.967%\n",
            "| Epoch [ 19/ 50] Iter[151/  6]\t\tLoss: 21.9966 Acc@1: 44.412%\n",
            "| Epoch [ 19/ 50] Iter[201/  6]\t\tLoss: 29.8491 Acc@1: 44.201%\n",
            "| Epoch [ 19/ 50] Iter[251/  6]\t\tLoss: 30.2479 Acc@1: 44.018%\n",
            "| Epoch [ 19/ 50] Iter[301/  6]\t\tLoss: 34.5619 Acc@1: 43.708%\n",
            "\n",
            "| Validation Epoch #19\t\t\tLoss: 35.3141 Acc@1: 41.78%\n",
            "lr = 0.0001\n",
            "| Epoch [ 20/ 50] Iter[  1/  6]\t\tLoss: 21.1132 Acc@1: 45.312%\n",
            "| Epoch [ 20/ 50] Iter[ 51/  6]\t\tLoss: 34.9466 Acc@1: 41.391%\n",
            "| Epoch [ 20/ 50] Iter[101/  6]\t\tLoss: 31.6105 Acc@1: 41.816%\n",
            "| Epoch [ 20/ 50] Iter[151/  6]\t\tLoss: 32.9645 Acc@1: 40.853%\n",
            "| Epoch [ 20/ 50] Iter[201/  6]\t\tLoss: 32.8203 Acc@1: 41.192%\n",
            "| Epoch [ 20/ 50] Iter[251/  6]\t\tLoss: 30.8599 Acc@1: 41.366%\n",
            "| Epoch [ 20/ 50] Iter[301/  6]\t\tLoss: 34.6724 Acc@1: 41.248%\n",
            "\n",
            "| Validation Epoch #20\t\t\tLoss: 29.0295 Acc@1: 41.05%\n",
            "lr = 0.0001\n",
            "| Epoch [ 21/ 50] Iter[  1/  6]\t\tLoss: 23.1725 Acc@1: 51.562%\n",
            "| Epoch [ 21/ 50] Iter[ 51/  6]\t\tLoss: 29.2444 Acc@1: 43.107%\n",
            "| Epoch [ 21/ 50] Iter[101/  6]\t\tLoss: 35.7947 Acc@1: 42.327%\n",
            "| Epoch [ 21/ 50] Iter[151/  6]\t\tLoss: 23.9885 Acc@1: 42.105%\n",
            "| Epoch [ 21/ 50] Iter[201/  6]\t\tLoss: 25.7895 Acc@1: 42.141%\n",
            "| Epoch [ 21/ 50] Iter[251/  6]\t\tLoss: 28.6849 Acc@1: 42.312%\n",
            "| Epoch [ 21/ 50] Iter[301/  6]\t\tLoss: 23.8983 Acc@1: 42.608%\n",
            "\n",
            "| Validation Epoch #21\t\t\tLoss: 14.4832 Acc@1: 44.98%\n",
            "lr = 0.0001\n",
            "| Epoch [ 22/ 50] Iter[  1/  6]\t\tLoss: 24.0549 Acc@1: 50.000%\n",
            "| Epoch [ 22/ 50] Iter[ 51/  6]\t\tLoss: 24.0039 Acc@1: 45.496%\n",
            "| Epoch [ 22/ 50] Iter[101/  6]\t\tLoss: 16.1593 Acc@1: 47.246%\n",
            "| Epoch [ 22/ 50] Iter[151/  6]\t\tLoss: 16.0183 Acc@1: 46.616%\n",
            "| Epoch [ 22/ 50] Iter[201/  6]\t\tLoss: 17.7871 Acc@1: 47.256%\n",
            "| Epoch [ 22/ 50] Iter[251/  6]\t\tLoss: 12.2700 Acc@1: 47.927%\n",
            "| Epoch [ 22/ 50] Iter[301/  6]\t\tLoss: 14.7267 Acc@1: 48.614%\n",
            "\n",
            "| Validation Epoch #22\t\t\tLoss: 7.7330 Acc@1: 51.31%\n",
            "lr = 0.0001\n",
            "Best RMSE is of : 13.504455354478624 for epoch : 22 ERROR TEST =  (13.504455354478624, tensor(51.3100))\n",
            "| Epoch [ 23/ 50] Iter[  1/  6]\t\tLoss: 19.3901 Acc@1: 42.188%\n",
            "| Epoch [ 23/ 50] Iter[ 51/  6]\t\tLoss: 12.1704 Acc@1: 52.298%\n",
            "| Epoch [ 23/ 50] Iter[101/  6]\t\tLoss: 9.3776 Acc@1: 52.119%\n",
            "| Epoch [ 23/ 50] Iter[151/  6]\t\tLoss: 13.5547 Acc@1: 52.566%\n",
            "| Epoch [ 23/ 50] Iter[201/  6]\t\tLoss: 10.8501 Acc@1: 52.169%\n",
            "| Epoch [ 23/ 50] Iter[251/  6]\t\tLoss: 8.9640 Acc@1: 52.036%\n",
            "| Epoch [ 23/ 50] Iter[301/  6]\t\tLoss: 10.7155 Acc@1: 52.149%\n",
            "\n",
            "| Validation Epoch #23\t\t\tLoss: 13.3314 Acc@1: 51.27%\n",
            "lr = 0.0001\n",
            "Best RMSE is of : 12.952316738941052 for epoch : 23 ERROR TEST =  (12.952316738941052, tensor(51.2664))\n",
            "| Epoch [ 24/ 50] Iter[  1/  6]\t\tLoss: 9.6532 Acc@1: 56.250%\n",
            "| Epoch [ 24/ 50] Iter[ 51/  6]\t\tLoss: 14.9549 Acc@1: 51.409%\n",
            "| Epoch [ 24/ 50] Iter[101/  6]\t\tLoss: 15.2744 Acc@1: 50.959%\n",
            "| Epoch [ 24/ 50] Iter[151/  6]\t\tLoss: 12.4969 Acc@1: 50.248%\n",
            "| Epoch [ 24/ 50] Iter[201/  6]\t\tLoss: 15.6855 Acc@1: 49.922%\n",
            "| Epoch [ 24/ 50] Iter[251/  6]\t\tLoss: 15.2697 Acc@1: 49.490%\n",
            "| Epoch [ 24/ 50] Iter[301/  6]\t\tLoss: 19.2622 Acc@1: 49.112%\n",
            "\n",
            "| Validation Epoch #24\t\t\tLoss: 6.5903 Acc@1: 46.58%\n",
            "lr = 0.0001\n",
            "| Epoch [ 25/ 50] Iter[  1/  6]\t\tLoss: 17.3938 Acc@1: 43.750%\n",
            "| Epoch [ 25/ 50] Iter[ 51/  6]\t\tLoss: 16.8156 Acc@1: 45.282%\n",
            "| Epoch [ 25/ 50] Iter[101/  6]\t\tLoss: 25.1795 Acc@1: 45.684%\n",
            "| Epoch [ 25/ 50] Iter[151/  6]\t\tLoss: 20.8137 Acc@1: 45.830%\n",
            "| Epoch [ 25/ 50] Iter[201/  6]\t\tLoss: 24.0965 Acc@1: 45.553%\n",
            "| Epoch [ 25/ 50] Iter[251/  6]\t\tLoss: 18.2561 Acc@1: 45.549%\n",
            "| Epoch [ 25/ 50] Iter[301/  6]\t\tLoss: 21.7185 Acc@1: 45.505%\n",
            "\n",
            "| Validation Epoch #25\t\t\tLoss: 11.9424 Acc@1: 45.20%\n",
            "lr = 0.0001\n",
            "| Epoch [ 26/ 50] Iter[  1/  6]\t\tLoss: 20.2403 Acc@1: 40.625%\n",
            "| Epoch [ 26/ 50] Iter[ 51/  6]\t\tLoss: 26.4728 Acc@1: 44.853%\n",
            "| Epoch [ 26/ 50] Iter[101/  6]\t\tLoss: 23.0971 Acc@1: 45.019%\n",
            "| Epoch [ 26/ 50] Iter[151/  6]\t\tLoss: 14.5294 Acc@1: 45.147%\n",
            "| Epoch [ 26/ 50] Iter[201/  6]\t\tLoss: 16.4414 Acc@1: 45.243%\n",
            "| Epoch [ 26/ 50] Iter[251/  6]\t\tLoss: 24.9302 Acc@1: 45.481%\n",
            "| Epoch [ 26/ 50] Iter[301/  6]\t\tLoss: 19.8068 Acc@1: 45.484%\n",
            "\n",
            "| Validation Epoch #26\t\t\tLoss: 26.9642 Acc@1: 44.41%\n",
            "lr = 0.0001\n",
            "| Epoch [ 27/ 50] Iter[  1/  6]\t\tLoss: 24.3546 Acc@1: 39.062%\n",
            "| Epoch [ 27/ 50] Iter[ 51/  6]\t\tLoss: 19.3807 Acc@1: 44.301%\n",
            "| Epoch [ 27/ 50] Iter[101/  6]\t\tLoss: 26.6623 Acc@1: 44.276%\n",
            "| Epoch [ 27/ 50] Iter[151/  6]\t\tLoss: 25.4058 Acc@1: 44.081%\n",
            "| Epoch [ 27/ 50] Iter[201/  6]\t\tLoss: 18.4622 Acc@1: 44.279%\n",
            "| Epoch [ 27/ 50] Iter[251/  6]\t\tLoss: 15.8265 Acc@1: 44.628%\n",
            "| Epoch [ 27/ 50] Iter[301/  6]\t\tLoss: 15.8552 Acc@1: 44.627%\n",
            "\n",
            "| Validation Epoch #27\t\t\tLoss: 20.9405 Acc@1: 45.07%\n",
            "lr = 0.0001\n",
            "| Epoch [ 28/ 50] Iter[  1/  6]\t\tLoss: 16.4318 Acc@1: 43.750%\n",
            "| Epoch [ 28/ 50] Iter[ 51/  6]\t\tLoss: 18.9718 Acc@1: 44.210%\n",
            "| Epoch [ 28/ 50] Iter[101/  6]\t\tLoss: 15.3446 Acc@1: 46.024%\n",
            "| Epoch [ 28/ 50] Iter[151/  6]\t\tLoss: 14.9165 Acc@1: 47.123%\n",
            "| Epoch [ 28/ 50] Iter[201/  6]\t\tLoss: 7.6605 Acc@1: 48.220%\n",
            "| Epoch [ 28/ 50] Iter[251/  6]\t\tLoss: 10.4943 Acc@1: 49.346%\n",
            "| Epoch [ 28/ 50] Iter[301/  6]\t\tLoss: 8.4378 Acc@1: 50.093%\n",
            "\n",
            "| Validation Epoch #28\t\t\tLoss: 6.4539 Acc@1: 53.89%\n",
            "lr = 0.0001\n",
            "Best RMSE is of : 9.807181464301216 for epoch : 28 ERROR TEST =  (9.807181464301216, tensor(53.8865))\n",
            "| Epoch [ 29/ 50] Iter[  1/  6]\t\tLoss: 6.9573 Acc@1: 56.250%\n",
            "| Epoch [ 29/ 50] Iter[ 51/  6]\t\tLoss: 13.1569 Acc@1: 54.933%\n",
            "| Epoch [ 29/ 50] Iter[101/  6]\t\tLoss: 8.5704 Acc@1: 55.523%\n",
            "| Epoch [ 29/ 50] Iter[151/  6]\t\tLoss: 7.6837 Acc@1: 55.070%\n",
            "| Epoch [ 29/ 50] Iter[201/  6]\t\tLoss: 9.3931 Acc@1: 54.781%\n",
            "| Epoch [ 29/ 50] Iter[251/  6]\t\tLoss: 10.9843 Acc@1: 54.407%\n",
            "| Epoch [ 29/ 50] Iter[301/  6]\t\tLoss: 7.6421 Acc@1: 54.158%\n",
            "\n",
            "| Validation Epoch #29\t\t\tLoss: 14.7600 Acc@1: 50.73%\n",
            "lr = 0.0001\n",
            "| Epoch [ 30/ 50] Iter[  1/  6]\t\tLoss: 8.6267 Acc@1: 54.688%\n",
            "| Epoch [ 30/ 50] Iter[ 51/  6]\t\tLoss: 11.8812 Acc@1: 51.317%\n",
            "| Epoch [ 30/ 50] Iter[101/  6]\t\tLoss: 10.8680 Acc@1: 50.449%\n",
            "| Epoch [ 30/ 50] Iter[151/  6]\t\tLoss: 16.7776 Acc@1: 49.555%\n",
            "| Epoch [ 30/ 50] Iter[201/  6]\t\tLoss: 14.5970 Acc@1: 49.238%\n",
            "| Epoch [ 30/ 50] Iter[251/  6]\t\tLoss: 14.0962 Acc@1: 48.531%\n",
            "| Epoch [ 30/ 50] Iter[301/  6]\t\tLoss: 16.5099 Acc@1: 48.116%\n",
            "\n",
            "| Validation Epoch #30\t\t\tLoss: 16.8063 Acc@1: 44.25%\n",
            "lr = 0.0001\n",
            "| Epoch [ 31/ 50] Iter[  1/  6]\t\tLoss: 19.8868 Acc@1: 40.625%\n",
            "| Epoch [ 31/ 50] Iter[ 51/  6]\t\tLoss: 19.7345 Acc@1: 45.067%\n",
            "| Epoch [ 31/ 50] Iter[101/  6]\t\tLoss: 16.9794 Acc@1: 44.539%\n",
            "| Epoch [ 31/ 50] Iter[151/  6]\t\tLoss: 15.3461 Acc@1: 44.723%\n",
            "| Epoch [ 31/ 50] Iter[201/  6]\t\tLoss: 12.7525 Acc@1: 44.776%\n",
            "| Epoch [ 31/ 50] Iter[251/  6]\t\tLoss: 13.8720 Acc@1: 44.578%\n",
            "| Epoch [ 31/ 50] Iter[301/  6]\t\tLoss: 16.3034 Acc@1: 44.612%\n",
            "\n",
            "| Validation Epoch #31\t\t\tLoss: 16.1931 Acc@1: 45.15%\n",
            "lr = 0.0001\n",
            "| Epoch [ 32/ 50] Iter[  1/  6]\t\tLoss: 16.6445 Acc@1: 34.375%\n",
            "| Epoch [ 32/ 50] Iter[ 51/  6]\t\tLoss: 10.0481 Acc@1: 46.446%\n",
            "| Epoch [ 32/ 50] Iter[101/  6]\t\tLoss: 17.5616 Acc@1: 46.442%\n",
            "| Epoch [ 32/ 50] Iter[151/  6]\t\tLoss: 17.1513 Acc@1: 47.517%\n",
            "| Epoch [ 32/ 50] Iter[201/  6]\t\tLoss: 12.9623 Acc@1: 48.430%\n",
            "| Epoch [ 32/ 50] Iter[251/  6]\t\tLoss: 11.7682 Acc@1: 48.518%\n",
            "| Epoch [ 32/ 50] Iter[301/  6]\t\tLoss: 7.6494 Acc@1: 49.014%\n",
            "\n",
            "| Validation Epoch #32\t\t\tLoss: 8.0595 Acc@1: 51.32%\n",
            "lr = 0.0001\n",
            "| Epoch [ 33/ 50] Iter[  1/  6]\t\tLoss: 12.9112 Acc@1: 45.312%\n",
            "| Epoch [ 33/ 50] Iter[ 51/  6]\t\tLoss: 8.3543 Acc@1: 51.195%\n",
            "| Epoch [ 33/ 50] Iter[101/  6]\t\tLoss: 8.0430 Acc@1: 51.485%\n",
            "| Epoch [ 33/ 50] Iter[151/  6]\t\tLoss: 9.4847 Acc@1: 51.738%\n",
            "| Epoch [ 33/ 50] Iter[201/  6]\t\tLoss: 9.0045 Acc@1: 51.803%\n",
            "| Epoch [ 33/ 50] Iter[251/  6]\t\tLoss: 12.9446 Acc@1: 51.562%\n",
            "| Epoch [ 33/ 50] Iter[301/  6]\t\tLoss: 12.8177 Acc@1: 51.381%\n",
            "\n",
            "| Validation Epoch #33\t\t\tLoss: 5.5191 Acc@1: 49.62%\n",
            "lr = 0.0001\n",
            "| Epoch [ 34/ 50] Iter[  1/  6]\t\tLoss: 11.0423 Acc@1: 48.438%\n",
            "| Epoch [ 34/ 50] Iter[ 51/  6]\t\tLoss: 9.5973 Acc@1: 49.479%\n",
            "| Epoch [ 34/ 50] Iter[101/  6]\t\tLoss: 11.7079 Acc@1: 49.087%\n",
            "| Epoch [ 34/ 50] Iter[151/  6]\t\tLoss: 11.7989 Acc@1: 49.834%\n",
            "| Epoch [ 34/ 50] Iter[201/  6]\t\tLoss: 9.7462 Acc@1: 49.285%\n",
            "| Epoch [ 34/ 50] Iter[251/  6]\t\tLoss: 12.7149 Acc@1: 49.359%\n",
            "| Epoch [ 34/ 50] Iter[301/  6]\t\tLoss: 19.9766 Acc@1: 49.387%\n",
            "\n",
            "| Validation Epoch #34\t\t\tLoss: 8.0116 Acc@1: 49.85%\n",
            "lr = 0.0001\n",
            "| Epoch [ 35/ 50] Iter[  1/  6]\t\tLoss: 10.6891 Acc@1: 62.500%\n",
            "| Epoch [ 35/ 50] Iter[ 51/  6]\t\tLoss: 14.7288 Acc@1: 48.836%\n",
            "| Epoch [ 35/ 50] Iter[101/  6]\t\tLoss: 15.6366 Acc@1: 49.752%\n",
            "| Epoch [ 35/ 50] Iter[151/  6]\t\tLoss: 11.1619 Acc@1: 49.845%\n",
            "| Epoch [ 35/ 50] Iter[201/  6]\t\tLoss: 18.8304 Acc@1: 49.254%\n",
            "| Epoch [ 35/ 50] Iter[251/  6]\t\tLoss: 16.5793 Acc@1: 49.054%\n",
            "| Epoch [ 35/ 50] Iter[301/  6]\t\tLoss: 12.6759 Acc@1: 48.848%\n",
            "\n",
            "| Validation Epoch #35\t\t\tLoss: 14.0040 Acc@1: 48.20%\n",
            "lr = 0.0001\n",
            "| Epoch [ 36/ 50] Iter[  1/  6]\t\tLoss: 16.4575 Acc@1: 51.562%\n",
            "| Epoch [ 36/ 50] Iter[ 51/  6]\t\tLoss: 19.8230 Acc@1: 47.335%\n",
            "| Epoch [ 36/ 50] Iter[101/  6]\t\tLoss: 11.9247 Acc@1: 47.618%\n",
            "| Epoch [ 36/ 50] Iter[151/  6]\t\tLoss: 17.0812 Acc@1: 47.434%\n",
            "| Epoch [ 36/ 50] Iter[201/  6]\t\tLoss: 20.3154 Acc@1: 47.831%\n",
            "| Epoch [ 36/ 50] Iter[251/  6]\t\tLoss: 14.3297 Acc@1: 48.325%\n",
            "| Epoch [ 36/ 50] Iter[301/  6]\t\tLoss: 15.3823 Acc@1: 48.313%\n",
            "\n",
            "| Validation Epoch #36\t\t\tLoss: 7.4488 Acc@1: 49.46%\n",
            "lr = 1e-05\n",
            "| Epoch [ 37/ 50] Iter[  1/  6]\t\tLoss: 9.6866 Acc@1: 60.938%\n",
            "| Epoch [ 37/ 50] Iter[ 51/  6]\t\tLoss: 17.8213 Acc@1: 50.123%\n",
            "| Epoch [ 37/ 50] Iter[101/  6]\t\tLoss: 15.1922 Acc@1: 50.124%\n",
            "| Epoch [ 37/ 50] Iter[151/  6]\t\tLoss: 13.3321 Acc@1: 49.545%\n",
            "| Epoch [ 37/ 50] Iter[201/  6]\t\tLoss: 10.2700 Acc@1: 49.526%\n",
            "| Epoch [ 37/ 50] Iter[251/  6]\t\tLoss: 15.4490 Acc@1: 49.471%\n",
            "| Epoch [ 37/ 50] Iter[301/  6]\t\tLoss: 17.7971 Acc@1: 49.595%\n",
            "\n",
            "| Validation Epoch #37\t\t\tLoss: 13.5899 Acc@1: 49.78%\n",
            "lr = 1e-05\n",
            "| Epoch [ 38/ 50] Iter[  1/  6]\t\tLoss: 15.1597 Acc@1: 50.000%\n",
            "| Epoch [ 38/ 50] Iter[ 51/  6]\t\tLoss: 15.2241 Acc@1: 49.571%\n",
            "| Epoch [ 38/ 50] Iter[101/  6]\t\tLoss: 14.0825 Acc@1: 49.660%\n",
            "| Epoch [ 38/ 50] Iter[151/  6]\t\tLoss: 17.9302 Acc@1: 49.669%\n",
            "| Epoch [ 38/ 50] Iter[201/  6]\t\tLoss: 10.3253 Acc@1: 49.611%\n",
            "| Epoch [ 38/ 50] Iter[251/  6]\t\tLoss: 15.6373 Acc@1: 49.651%\n",
            "| Epoch [ 38/ 50] Iter[301/  6]\t\tLoss: 13.2750 Acc@1: 49.720%\n",
            "\n",
            "| Validation Epoch #38\t\t\tLoss: 14.6609 Acc@1: 49.97%\n",
            "lr = 1e-05\n",
            "| Epoch [ 39/ 50] Iter[  1/  6]\t\tLoss: 9.3251 Acc@1: 50.000%\n",
            "| Epoch [ 39/ 50] Iter[ 51/  6]\t\tLoss: 13.3903 Acc@1: 49.295%\n",
            "| Epoch [ 39/ 50] Iter[101/  6]\t\tLoss: 10.3504 Acc@1: 50.186%\n",
            "| Epoch [ 39/ 50] Iter[151/  6]\t\tLoss: 11.6027 Acc@1: 50.052%\n",
            "| Epoch [ 39/ 50] Iter[201/  6]\t\tLoss: 14.3786 Acc@1: 50.179%\n",
            "| Epoch [ 39/ 50] Iter[251/  6]\t\tLoss: 13.9344 Acc@1: 50.380%\n",
            "| Epoch [ 39/ 50] Iter[301/  6]\t\tLoss: 12.2974 Acc@1: 50.374%\n",
            "\n",
            "| Validation Epoch #39\t\t\tLoss: 21.3380 Acc@1: 50.19%\n",
            "lr = 1e-05\n",
            "| Epoch [ 40/ 50] Iter[  1/  6]\t\tLoss: 16.4370 Acc@1: 45.312%\n",
            "| Epoch [ 40/ 50] Iter[ 51/  6]\t\tLoss: 12.5042 Acc@1: 49.326%\n",
            "| Epoch [ 40/ 50] Iter[101/  6]\t\tLoss: 10.7513 Acc@1: 50.139%\n",
            "| Epoch [ 40/ 50] Iter[151/  6]\t\tLoss: 11.5390 Acc@1: 50.217%\n",
            "| Epoch [ 40/ 50] Iter[201/  6]\t\tLoss: 13.9765 Acc@1: 50.350%\n",
            "| Epoch [ 40/ 50] Iter[251/  6]\t\tLoss: 12.5144 Acc@1: 50.510%\n",
            "| Epoch [ 40/ 50] Iter[301/  6]\t\tLoss: 8.9660 Acc@1: 50.592%\n",
            "\n",
            "| Validation Epoch #40\t\t\tLoss: 15.9017 Acc@1: 50.38%\n",
            "lr = 1e-05\n",
            "| Epoch [ 41/ 50] Iter[  1/  6]\t\tLoss: 13.8352 Acc@1: 40.625%\n",
            "| Epoch [ 41/ 50] Iter[ 51/  6]\t\tLoss: 11.4351 Acc@1: 50.735%\n",
            "| Epoch [ 41/ 50] Iter[101/  6]\t\tLoss: 10.3147 Acc@1: 50.835%\n",
            "| Epoch [ 41/ 50] Iter[151/  6]\t\tLoss: 12.9299 Acc@1: 50.435%\n",
            "| Epoch [ 41/ 50] Iter[201/  6]\t\tLoss: 12.6744 Acc@1: 50.606%\n",
            "| Epoch [ 41/ 50] Iter[251/  6]\t\tLoss: 13.0656 Acc@1: 50.616%\n",
            "| Epoch [ 41/ 50] Iter[301/  6]\t\tLoss: 13.2074 Acc@1: 50.680%\n",
            "\n",
            "| Validation Epoch #41\t\t\tLoss: 6.5263 Acc@1: 50.89%\n",
            "lr = 1e-05\n",
            "| Epoch [ 42/ 50] Iter[  1/  6]\t\tLoss: 10.8495 Acc@1: 50.000%\n",
            "| Epoch [ 42/ 50] Iter[ 51/  6]\t\tLoss: 9.6992 Acc@1: 51.011%\n",
            "| Epoch [ 42/ 50] Iter[101/  6]\t\tLoss: 14.5372 Acc@1: 51.284%\n",
            "| Epoch [ 42/ 50] Iter[151/  6]\t\tLoss: 9.0942 Acc@1: 51.262%\n",
            "| Epoch [ 42/ 50] Iter[201/  6]\t\tLoss: 10.0085 Acc@1: 51.228%\n",
            "| Epoch [ 42/ 50] Iter[251/  6]\t\tLoss: 9.6724 Acc@1: 50.990%\n",
            "| Epoch [ 42/ 50] Iter[301/  6]\t\tLoss: 11.4804 Acc@1: 51.106%\n",
            "\n",
            "| Validation Epoch #42\t\t\tLoss: 10.5917 Acc@1: 51.03%\n",
            "lr = 1e-05\n",
            "| Epoch [ 43/ 50] Iter[  1/  6]\t\tLoss: 11.3341 Acc@1: 53.125%\n",
            "| Epoch [ 43/ 50] Iter[ 51/  6]\t\tLoss: 9.8619 Acc@1: 51.317%\n",
            "| Epoch [ 43/ 50] Iter[101/  6]\t\tLoss: 11.5267 Acc@1: 50.990%\n",
            "| Epoch [ 43/ 50] Iter[151/  6]\t\tLoss: 9.1199 Acc@1: 50.797%\n",
            "| Epoch [ 43/ 50] Iter[201/  6]\t\tLoss: 9.9842 Acc@1: 51.049%\n",
            "| Epoch [ 43/ 50] Iter[251/  6]\t\tLoss: 11.4567 Acc@1: 51.257%\n",
            "| Epoch [ 43/ 50] Iter[301/  6]\t\tLoss: 13.0513 Acc@1: 51.376%\n",
            "\n",
            "| Validation Epoch #43\t\t\tLoss: 14.7742 Acc@1: 51.41%\n",
            "lr = 1e-05\n",
            "| Epoch [ 44/ 50] Iter[  1/  6]\t\tLoss: 11.4801 Acc@1: 51.562%\n",
            "| Epoch [ 44/ 50] Iter[ 51/  6]\t\tLoss: 13.7371 Acc@1: 51.501%\n",
            "| Epoch [ 44/ 50] Iter[101/  6]\t\tLoss: 9.3984 Acc@1: 51.578%\n",
            "| Epoch [ 44/ 50] Iter[151/  6]\t\tLoss: 8.5046 Acc@1: 51.428%\n",
            "| Epoch [ 44/ 50] Iter[201/  6]\t\tLoss: 12.5206 Acc@1: 51.415%\n",
            "| Epoch [ 44/ 50] Iter[251/  6]\t\tLoss: 10.7241 Acc@1: 51.787%\n",
            "| Epoch [ 44/ 50] Iter[301/  6]\t\tLoss: 18.0246 Acc@1: 51.666%\n",
            "\n",
            "| Validation Epoch #44\t\t\tLoss: 16.2663 Acc@1: 51.47%\n",
            "lr = 1.0000000000000002e-06\n",
            "| Epoch [ 45/ 50] Iter[  1/  6]\t\tLoss: 8.2198 Acc@1: 59.375%\n",
            "| Epoch [ 45/ 50] Iter[ 51/  6]\t\tLoss: 8.6556 Acc@1: 51.225%\n",
            "| Epoch [ 45/ 50] Iter[101/  6]\t\tLoss: 8.6154 Acc@1: 51.454%\n",
            "| Epoch [ 45/ 50] Iter[151/  6]\t\tLoss: 10.9129 Acc@1: 51.024%\n",
            "| Epoch [ 45/ 50] Iter[201/  6]\t\tLoss: 9.5666 Acc@1: 51.244%\n",
            "| Epoch [ 45/ 50] Iter[251/  6]\t\tLoss: 8.6020 Acc@1: 51.606%\n",
            "| Epoch [ 45/ 50] Iter[301/  6]\t\tLoss: 12.6777 Acc@1: 51.620%\n",
            "\n",
            "| Validation Epoch #45\t\t\tLoss: 10.6928 Acc@1: 51.54%\n",
            "lr = 1.0000000000000002e-06\n",
            "| Epoch [ 46/ 50] Iter[  1/  6]\t\tLoss: 8.2803 Acc@1: 54.688%\n",
            "| Epoch [ 46/ 50] Iter[ 51/  6]\t\tLoss: 8.7627 Acc@1: 51.900%\n",
            "| Epoch [ 46/ 50] Iter[101/  6]\t\tLoss: 9.1103 Acc@1: 51.191%\n",
            "| Epoch [ 46/ 50] Iter[151/  6]\t\tLoss: 8.4634 Acc@1: 51.211%\n",
            "| Epoch [ 46/ 50] Iter[201/  6]\t\tLoss: 12.9942 Acc@1: 51.562%\n",
            "| Epoch [ 46/ 50] Iter[251/  6]\t\tLoss: 8.7288 Acc@1: 51.799%\n",
            "| Epoch [ 46/ 50] Iter[301/  6]\t\tLoss: 11.5717 Acc@1: 51.874%\n",
            "\n",
            "| Validation Epoch #46\t\t\tLoss: 7.9072 Acc@1: 51.59%\n",
            "lr = 1.0000000000000002e-06\n",
            "| Epoch [ 47/ 50] Iter[  1/  6]\t\tLoss: 11.5060 Acc@1: 45.312%\n",
            "| Epoch [ 47/ 50] Iter[ 51/  6]\t\tLoss: 10.7282 Acc@1: 53.370%\n",
            "| Epoch [ 47/ 50] Iter[101/  6]\t\tLoss: 10.2073 Acc@1: 51.965%\n",
            "| Epoch [ 47/ 50] Iter[151/  6]\t\tLoss: 6.3279 Acc@1: 52.204%\n",
            "| Epoch [ 47/ 50] Iter[201/  6]\t\tLoss: 9.8124 Acc@1: 52.138%\n",
            "| Epoch [ 47/ 50] Iter[251/  6]\t\tLoss: 9.7359 Acc@1: 52.253%\n",
            "| Epoch [ 47/ 50] Iter[301/  6]\t\tLoss: 15.0945 Acc@1: 52.082%\n",
            "\n",
            "| Validation Epoch #47\t\t\tLoss: 11.3491 Acc@1: 51.63%\n",
            "lr = 1.0000000000000002e-06\n",
            "| Epoch [ 48/ 50] Iter[  1/  6]\t\tLoss: 14.4223 Acc@1: 50.000%\n",
            "| Epoch [ 48/ 50] Iter[ 51/  6]\t\tLoss: 11.9290 Acc@1: 52.022%\n",
            "| Epoch [ 48/ 50] Iter[101/  6]\t\tLoss: 8.5788 Acc@1: 51.856%\n",
            "| Epoch [ 48/ 50] Iter[151/  6]\t\tLoss: 10.6136 Acc@1: 52.028%\n",
            "| Epoch [ 48/ 50] Iter[201/  6]\t\tLoss: 11.1500 Acc@1: 51.943%\n",
            "| Epoch [ 48/ 50] Iter[251/  6]\t\tLoss: 12.3432 Acc@1: 51.830%\n",
            "| Epoch [ 48/ 50] Iter[301/  6]\t\tLoss: 8.0775 Acc@1: 51.806%\n",
            "\n",
            "| Validation Epoch #48\t\t\tLoss: 15.0252 Acc@1: 51.69%\n",
            "lr = 1.0000000000000002e-06\n",
            "| Epoch [ 49/ 50] Iter[  1/  6]\t\tLoss: 8.7109 Acc@1: 54.688%\n",
            "| Epoch [ 49/ 50] Iter[ 51/  6]\t\tLoss: 9.6774 Acc@1: 51.562%\n",
            "| Epoch [ 49/ 50] Iter[101/  6]\t\tLoss: 10.3144 Acc@1: 51.764%\n",
            "| Epoch [ 49/ 50] Iter[151/  6]\t\tLoss: 10.2879 Acc@1: 51.707%\n",
            "| Epoch [ 49/ 50] Iter[201/  6]\t\tLoss: 13.9224 Acc@1: 51.943%\n",
            "| Epoch [ 49/ 50] Iter[251/  6]\t\tLoss: 9.5339 Acc@1: 52.079%\n",
            "| Epoch [ 49/ 50] Iter[301/  6]\t\tLoss: 9.2587 Acc@1: 51.957%\n",
            "\n",
            "| Validation Epoch #49\t\t\tLoss: 7.1972 Acc@1: 51.64%\n",
            "lr = 1.0000000000000002e-06\n",
            "Training Done!\n"
          ]
        }
      ],
      "source": [
        "print (\"Let us Train.\")\n",
        "EPOCHS = 50\n",
        "model = SentimentClassifier(768, NUM_CLASSES).to(device)\n",
        "model_test = SentimentClassifier(768, NUM_CLASSES).to(device)\n",
        "best_error = float('inf')\n",
        "LEARNING_RATE = 1e-3\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau,StepLR\n",
        "\n",
        "traindataset = EmbeddingDataset(train_embeddings, train_y)\n",
        "trainloader = DataLoader(traindataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valdataset = EmbeddingDataset(val_embeddings, val_y)\n",
        "valloader = DataLoader(valdataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "#lr_scheduler =  StepLR(optimizer, step_size=20, gamma=0.1)\n",
        "lr_scheduler = ReduceLROnPlateau(optimizer, 'min', patience=7)\n",
        "\n",
        "train_history = []\n",
        "val_history = []\n",
        "print('----------------------------------------------------------------------------------------------------')\n",
        "print('3/ training model 1')\n",
        "print('----------------------------------------------------------------------------------------------------')\n",
        "\n",
        "iter =0\n",
        "for epoch in range(EPOCHS):\n",
        "    loss = train(model, trainloader, optimizer, epoch)\n",
        "    train_history.append(loss)\n",
        "    #lr_scheduler.step()\n",
        "    val_loss= test(model, valloader,epoch)\n",
        "    val_history.append(val_loss[0])\n",
        "    lr_scheduler.step(val_loss[0])\n",
        "    print('lr =',get_lr(optimizer))\n",
        "\n",
        "    if val_loss[0] <best_error:\n",
        "        best_error=val_loss[0]\n",
        "        print('Best RMSE is of : ' + str(best_error), 'for epoch :', epoch,'ERROR TEST = ',val_loss)\n",
        "        #model_test.parameters()=model.state_dict()\n",
        "        model_test.load_state_dict(model.state_dict(), strict=True)\n",
        "\n",
        "print (\"Training Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUgqW43GU6ne"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-09-12T08:28:24.157868Z",
          "iopub.status.busy": "2024-09-12T08:28:24.157398Z",
          "iopub.status.idle": "2024-09-12T08:28:24.285993Z",
          "shell.execute_reply": "2024-09-12T08:28:24.284901Z",
          "shell.execute_reply.started": "2024-09-12T08:28:24.157825Z"
        },
        "id": "s8_Iuj66dBip",
        "outputId": "c4b9ca89-fb80-4c58-de6a-fb78c24d52fe",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================== \n",
            "Test set = \n",
            "\n",
            "| Validation Epoch #49\t\t\tLoss: 15.3432 Acc@1: 52.26%\n",
            "==================== \n"
          ]
        }
      ],
      "source": [
        "testdataset = TextDataset(test_x,test_y, tokenizer, MAX_LENGTH)\n",
        "testloader = DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "bert = BertModel.from_pretrained(PRETRAINED_MODEL).to(device)\n",
        "# Extract embeddings\n",
        "\n",
        "test_embeddings = extract_embeddings(bert, testloader, device)\n",
        "test_embeddings =test_embeddings.cpu()\n",
        "print('==================== ')\n",
        "print('Test set = ')\n",
        "testdataset = EmbeddingDataset(test_embeddings, test_y)\n",
        "testdataset = DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loss,accu = test(model, testdataset,epoch)\n",
        "print('==================== ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkD2ubONd_R2"
      },
      "source": [
        "## Questions\n",
        "\n",
        "**q0/  please analyse the dataset with differents classical machine learning model**\n",
        "\n",
        "**q1/  please perform a classification with differents classical machine learning model and analyse the performences**\n",
        "\n",
        "**q2/  please perform a classification with a MLP?**\n",
        "\n",
        "**q3/  please analyse all the performences and explain which is the best**\n",
        "\n",
        "**q4/  please use an LLM compare your performences to a LLM**\n",
        "\n",
        "**q5/  please explain why I choose a BERT embedding instead of the raw text**\n",
        "\n",
        "**q6/  please read the BERT paper and explain the BERT architecture**\n",
        "\n",
        "**q7/  please finetue with LORA an LLM to classify the sentiment (optional)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of KNN model is : 0.46066779852857953\n"
          ]
        }
      ],
      "source": [
        "## KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# Elbow method to find the best K\n",
        "k_range = range(1, 10)\n",
        "scores = []\n",
        "for k in k_range:\n",
        "    knn_model = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn_model.fit(train_embeddings, train_y)\n",
        "    knn_predictions = knn_model.predict(val_embeddings)\n",
        "    scores.append(accuracy_score(val_y, knn_predictions))\n",
        "plt.plot(k_range, scores)\n",
        "plt.xlabel('Value of K for KNN')\n",
        "plt.ylabel('Accuracy Score')\n",
        "plt.title('Accuracy Scores for Values of K of K-Nearest-Neighbors')\n",
        "plt.show()\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(train_embeddings, train_y)\n",
        "yhat = knn_model.predict(test_embeddings)\n",
        "print('Accuracy of KNN model is :',accuracy_score(test_y, yhat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUIRWJllOee8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression (newton-cg) Accuracy: 0.64\n",
            "Logistic Regression (lbfgs) Accuracy: 0.64\n",
            "Logistic Regression (sag) Accuracy: 0.64\n"
          ]
        }
      ],
      "source": [
        "## QUESTION 0 and 1 :\n",
        "import sklearn\n",
        "## LOGISTIC REGRESSION\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "for solver in ['newton-cg', 'lbfgs', 'sag', 'saga']:\n",
        "    logistic_model = LogisticRegression(max_iter=1000, solver=solver,C=10, random_state=42)\n",
        "    logistic_model.fit(train_embeddings, train_y)\n",
        "    logistic_predictions = logistic_model.predict(val_embeddings)\n",
        "    logistic_accuracy = accuracy_score(val_y, logistic_predictions)\n",
        "    print(f'Logistic Regression ({solver}) Accuracy: {logistic_accuracy:.2f}')\n",
        "    yhat = logistic_model.predict(test_embeddings)\n",
        "    acc = accuracy_score(test_y, yhat)\n",
        "    print(f'Logistic Regression ({solver}) Accuracy: {acc:.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision Tree Validation Accuracy: 0.42\n",
            "Decision Tree Test Accuracy: 0.42\n"
          ]
        }
      ],
      "source": [
        "## DECISION TREE\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
        "decision_tree_model.fit(train_embeddings, train_y)\n",
        "decision_tree_predictions = decision_tree_model.predict(val_embeddings)\n",
        "decision_tree_accuracy = accuracy_score(val_y, decision_tree_predictions)\n",
        "print(f'Decision Tree Validation Accuracy: {decision_tree_accuracy:.2f}')\n",
        "yhat = decision_tree_model.predict(test_embeddings)\n",
        "acc = accuracy_score(test_y, yhat)\n",
        "print(f'Decision Tree Test Accuracy: {acc:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Validation Accuracy: 0.54\n",
            "Random Forest Test Accuracy: 0.55\n"
          ]
        }
      ],
      "source": [
        "## RANDOM FOREST\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "random_forest_model = RandomForestClassifier(random_state=42, n_estimators=100,)\n",
        "random_forest_model.fit(train_embeddings, train_y)\n",
        "random_forest_predictions = random_forest_model.predict(val_embeddings)\n",
        "random_forest_accuracy = accuracy_score(val_y, random_forest_predictions)\n",
        "print(f'Random Forest Validation Accuracy: {random_forest_accuracy:.2f}')\n",
        "yhat = random_forest_model.predict(test_embeddings)\n",
        "acc = accuracy_score(test_y, yhat)\n",
        "print(f'Random Forest Test Accuracy: {acc:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM (linear, C=0.1) Validation Accuracy: 0.63\n",
            "SVM (linear, C=0.1) Test Accuracy: 0.64\n",
            "SVM (linear, C=1) Validation Accuracy: 0.65\n",
            "SVM (linear, C=1) Test Accuracy: 0.65\n"
          ]
        }
      ],
      "source": [
        "## SVM with parameters and hyperparameters\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "for kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
        "    for C in [0.1, 1, 10]:\n",
        "        svm_model = SVC(kernel=kernel, C=C, random_state=42)\n",
        "        svm_model.fit(train_embeddings, train_y)\n",
        "        svm_predictions = svm_model.predict(val_embeddings)\n",
        "        svm_accuracy = accuracy_score(val_y, svm_predictions)\n",
        "        print(f'SVM ({kernel}, C={C}) Validation Accuracy: {svm_accuracy:.2f}')\n",
        "        yhat = svm_model.predict(test_embeddings)\n",
        "        acc = accuracy_score(test_y, yhat)\n",
        "        print(f'SVM ({kernel}, C={C}) Test Accuracy: {acc:.2f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training MLP model.\n",
            "| Epoch [  0/ 50] Iter[  1/  6]\t\tLoss: 1.0902 Acc@1: 48.438%\n",
            "| Epoch [  0/ 50] Iter[ 51/  6]\t\tLoss: 1.6155 Acc@1: 36.458%\n",
            "| Epoch [  0/ 50] Iter[101/  6]\t\tLoss: 1.2153 Acc@1: 35.675%\n",
            "| Epoch [  0/ 50] Iter[151/  6]\t\tLoss: 1.1410 Acc@1: 34.065%\n",
            "| Epoch [  0/ 50] Iter[201/  6]\t\tLoss: 1.0866 Acc@1: 33.932%\n",
            "| Epoch [  0/ 50] Iter[251/  6]\t\tLoss: 1.1773 Acc@1: 34.350%\n",
            "| Epoch [  0/ 50] Iter[301/  6]\t\tLoss: 1.1318 Acc@1: 35.195%\n",
            "\n",
            "| Validation Epoch #0\t\t\tLoss: 1.1510 Acc@1: 40.57%\n",
            "lr = 0.001\n",
            "Best RMSE is of : 1.5389449000358582 for epoch : 0 ERROR TEST =  (1.5389449000358582, tensor(40.5677))\n",
            "| Epoch [  1/ 50] Iter[  1/  6]\t\tLoss: 1.0758 Acc@1: 43.750%\n",
            "| Epoch [  1/ 50] Iter[ 51/  6]\t\tLoss: 1.0999 Acc@1: 40.594%\n",
            "| Epoch [  1/ 50] Iter[101/  6]\t\tLoss: 1.0534 Acc@1: 40.780%\n",
            "| Epoch [  1/ 50] Iter[151/  6]\t\tLoss: 1.0788 Acc@1: 40.625%\n",
            "| Epoch [  1/ 50] Iter[201/  6]\t\tLoss: 1.1012 Acc@1: 40.229%\n",
            "| Epoch [  1/ 50] Iter[251/  6]\t\tLoss: 1.1200 Acc@1: 40.251%\n",
            "| Epoch [  1/ 50] Iter[301/  6]\t\tLoss: 1.0804 Acc@1: 40.485%\n",
            "\n",
            "| Validation Epoch #1\t\t\tLoss: 0.9727 Acc@1: 40.57%\n",
            "lr = 0.001\n",
            "Best RMSE is of : 1.3695947820389713 for epoch : 1 ERROR TEST =  (1.3695947820389713, tensor(40.5677))\n",
            "| Epoch [  2/ 50] Iter[  1/  6]\t\tLoss: 1.0986 Acc@1: 51.562%\n",
            "| Epoch [  2/ 50] Iter[ 51/  6]\t\tLoss: 1.0818 Acc@1: 41.054%\n",
            "| Epoch [  2/ 50] Iter[101/  6]\t\tLoss: 1.1009 Acc@1: 40.517%\n",
            "| Epoch [  2/ 50] Iter[151/  6]\t\tLoss: 1.0776 Acc@1: 40.356%\n",
            "| Epoch [  2/ 50] Iter[201/  6]\t\tLoss: 1.0942 Acc@1: 40.392%\n",
            "| Epoch [  2/ 50] Iter[251/  6]\t\tLoss: 1.0832 Acc@1: 40.563%\n",
            "| Epoch [  2/ 50] Iter[301/  6]\t\tLoss: 1.3548 Acc@1: 40.469%\n",
            "\n",
            "| Validation Epoch #2\t\t\tLoss: 1.1211 Acc@1: 40.57%\n",
            "lr = 0.001\n",
            "| Epoch [  3/ 50] Iter[  1/  6]\t\tLoss: 1.1288 Acc@1: 34.375%\n",
            "| Epoch [  3/ 50] Iter[ 51/  6]\t\tLoss: 1.0957 Acc@1: 41.850%\n",
            "| Epoch [  3/ 50] Iter[101/  6]\t\tLoss: 1.1304 Acc@1: 41.507%\n",
            "| Epoch [  3/ 50] Iter[151/  6]\t\tLoss: 1.1410 Acc@1: 41.267%\n",
            "| Epoch [  3/ 50] Iter[201/  6]\t\tLoss: 1.0800 Acc@1: 40.944%\n",
            "| Epoch [  3/ 50] Iter[251/  6]\t\tLoss: 1.1021 Acc@1: 40.532%\n",
            "| Epoch [  3/ 50] Iter[301/  6]\t\tLoss: 1.0817 Acc@1: 40.386%\n",
            "\n",
            "| Validation Epoch #3\t\t\tLoss: 1.0973 Acc@1: 40.49%\n",
            "lr = 0.001\n",
            "Best RMSE is of : 1.3177944565260853 for epoch : 3 ERROR TEST =  (1.3177944565260853, tensor(40.4949))\n",
            "| Epoch [  4/ 50] Iter[  1/  6]\t\tLoss: 1.0774 Acc@1: 42.188%\n",
            "| Epoch [  4/ 50] Iter[ 51/  6]\t\tLoss: 1.0986 Acc@1: 40.625%\n",
            "| Epoch [  4/ 50] Iter[101/  6]\t\tLoss: 1.0732 Acc@1: 39.898%\n",
            "| Epoch [  4/ 50] Iter[151/  6]\t\tLoss: 1.1446 Acc@1: 40.025%\n",
            "| Epoch [  4/ 50] Iter[201/  6]\t\tLoss: 1.1189 Acc@1: 40.229%\n",
            "| Epoch [  4/ 50] Iter[251/  6]\t\tLoss: 1.1005 Acc@1: 40.357%\n",
            "| Epoch [  4/ 50] Iter[301/  6]\t\tLoss: 1.0840 Acc@1: 40.376%\n",
            "\n",
            "| Validation Epoch #4\t\t\tLoss: 1.1327 Acc@1: 40.55%\n",
            "lr = 0.001\n",
            "| Epoch [  5/ 50] Iter[  1/  6]\t\tLoss: 1.8432 Acc@1: 35.938%\n",
            "| Epoch [  5/ 50] Iter[ 51/  6]\t\tLoss: 1.0671 Acc@1: 39.277%\n",
            "| Epoch [  5/ 50] Iter[101/  6]\t\tLoss: 1.0639 Acc@1: 40.439%\n",
            "| Epoch [  5/ 50] Iter[151/  6]\t\tLoss: 1.1389 Acc@1: 40.387%\n",
            "| Epoch [  5/ 50] Iter[201/  6]\t\tLoss: 1.0716 Acc@1: 40.314%\n",
            "| Epoch [  5/ 50] Iter[251/  6]\t\tLoss: 1.1042 Acc@1: 40.096%\n",
            "| Epoch [  5/ 50] Iter[301/  6]\t\tLoss: 1.0755 Acc@1: 40.339%\n",
            "\n",
            "| Validation Epoch #5\t\t\tLoss: 1.1242 Acc@1: 40.51%\n",
            "lr = 0.001\n",
            "| Epoch [  6/ 50] Iter[  1/  6]\t\tLoss: 1.1146 Acc@1: 39.062%\n",
            "| Epoch [  6/ 50] Iter[ 51/  6]\t\tLoss: 1.1021 Acc@1: 41.330%\n",
            "| Epoch [  6/ 50] Iter[101/  6]\t\tLoss: 1.1140 Acc@1: 41.538%\n",
            "| Epoch [  6/ 50] Iter[151/  6]\t\tLoss: 1.1152 Acc@1: 41.070%\n",
            "| Epoch [  6/ 50] Iter[201/  6]\t\tLoss: 1.1113 Acc@1: 40.415%\n",
            "| Epoch [  6/ 50] Iter[251/  6]\t\tLoss: 1.1153 Acc@1: 40.594%\n",
            "| Epoch [  6/ 50] Iter[301/  6]\t\tLoss: 1.0728 Acc@1: 40.360%\n",
            "\n",
            "| Validation Epoch #6\t\t\tLoss: 1.0582 Acc@1: 40.51%\n",
            "lr = 0.001\n",
            "| Epoch [  7/ 50] Iter[  1/  6]\t\tLoss: 1.0754 Acc@1: 39.062%\n",
            "| Epoch [  7/ 50] Iter[ 51/  6]\t\tLoss: 1.1645 Acc@1: 39.277%\n",
            "| Epoch [  7/ 50] Iter[101/  6]\t\tLoss: 1.1100 Acc@1: 35.179%\n",
            "| Epoch [  7/ 50] Iter[151/  6]\t\tLoss: 1.0885 Acc@1: 33.589%\n",
            "| Epoch [  7/ 50] Iter[201/  6]\t\tLoss: 1.0986 Acc@1: 33.776%\n",
            "| Epoch [  7/ 50] Iter[251/  6]\t\tLoss: 1.0968 Acc@1: 34.948%\n",
            "| Epoch [  7/ 50] Iter[301/  6]\t\tLoss: 1.0723 Acc@1: 35.938%\n",
            "\n",
            "| Validation Epoch #7\t\t\tLoss: 1.0396 Acc@1: 40.58%\n",
            "lr = 0.001\n",
            "| Epoch [  8/ 50] Iter[  1/  6]\t\tLoss: 1.0947 Acc@1: 37.500%\n",
            "| Epoch [  8/ 50] Iter[ 51/  6]\t\tLoss: 1.1224 Acc@1: 41.207%\n",
            "| Epoch [  8/ 50] Iter[101/  6]\t\tLoss: 1.0929 Acc@1: 39.944%\n",
            "| Epoch [  8/ 50] Iter[151/  6]\t\tLoss: 1.0808 Acc@1: 40.056%\n",
            "| Epoch [  8/ 50] Iter[201/  6]\t\tLoss: 1.1334 Acc@1: 40.190%\n",
            "| Epoch [  8/ 50] Iter[251/  6]\t\tLoss: 1.0942 Acc@1: 40.239%\n",
            "| Epoch [  8/ 50] Iter[301/  6]\t\tLoss: 1.0883 Acc@1: 38.403%\n",
            "\n",
            "| Validation Epoch #8\t\t\tLoss: 1.0567 Acc@1: 28.50%\n",
            "lr = 0.001\n",
            "| Epoch [  9/ 50] Iter[  1/  6]\t\tLoss: 1.1059 Acc@1: 34.375%\n",
            "| Epoch [  9/ 50] Iter[ 51/  6]\t\tLoss: 1.1008 Acc@1: 31.740%\n",
            "| Epoch [  9/ 50] Iter[101/  6]\t\tLoss: 1.0963 Acc@1: 36.340%\n",
            "| Epoch [  9/ 50] Iter[151/  6]\t\tLoss: 1.0798 Acc@1: 37.697%\n",
            "| Epoch [  9/ 50] Iter[201/  6]\t\tLoss: 1.1334 Acc@1: 38.060%\n",
            "| Epoch [  9/ 50] Iter[251/  6]\t\tLoss: 1.0833 Acc@1: 38.540%\n",
            "| Epoch [  9/ 50] Iter[301/  6]\t\tLoss: 1.1307 Acc@1: 38.979%\n",
            "\n",
            "| Validation Epoch #9\t\t\tLoss: 1.1544 Acc@1: 40.52%\n",
            "lr = 0.001\n",
            "| Epoch [ 10/ 50] Iter[  1/  6]\t\tLoss: 1.1181 Acc@1: 31.250%\n",
            "| Epoch [ 10/ 50] Iter[ 51/  6]\t\tLoss: 1.0710 Acc@1: 40.748%\n",
            "| Epoch [ 10/ 50] Iter[101/  6]\t\tLoss: 1.0301 Acc@1: 41.182%\n",
            "| Epoch [ 10/ 50] Iter[151/  6]\t\tLoss: 1.1090 Acc@1: 41.298%\n",
            "| Epoch [ 10/ 50] Iter[201/  6]\t\tLoss: 1.9669 Acc@1: 39.140%\n",
            "| Epoch [ 10/ 50] Iter[251/  6]\t\tLoss: 1.1400 Acc@1: 37.917%\n",
            "| Epoch [ 10/ 50] Iter[301/  6]\t\tLoss: 1.0518 Acc@1: 38.242%\n",
            "\n",
            "| Validation Epoch #10\t\t\tLoss: 1.1506 Acc@1: 40.54%\n",
            "lr = 0.001\n",
            "| Epoch [ 11/ 50] Iter[  1/  6]\t\tLoss: 1.0986 Acc@1: 34.375%\n",
            "| Epoch [ 11/ 50] Iter[ 51/  6]\t\tLoss: 1.0799 Acc@1: 38.419%\n",
            "| Epoch [ 11/ 50] Iter[101/  6]\t\tLoss: 1.1117 Acc@1: 39.418%\n",
            "| Epoch [ 11/ 50] Iter[151/  6]\t\tLoss: 1.1024 Acc@1: 40.087%\n",
            "| Epoch [ 11/ 50] Iter[201/  6]\t\tLoss: 1.0678 Acc@1: 40.485%\n",
            "| Epoch [ 11/ 50] Iter[251/  6]\t\tLoss: 1.1379 Acc@1: 40.488%\n",
            "| Epoch [ 11/ 50] Iter[301/  6]\t\tLoss: 1.1537 Acc@1: 40.506%\n",
            "\n",
            "| Validation Epoch #11\t\t\tLoss: 1.2069 Acc@1: 40.54%\n",
            "lr = 0.001\n",
            "| Epoch [ 12/ 50] Iter[  1/  6]\t\tLoss: 1.0781 Acc@1: 50.000%\n",
            "| Epoch [ 12/ 50] Iter[ 51/  6]\t\tLoss: 1.0730 Acc@1: 40.012%\n",
            "| Epoch [ 12/ 50] Iter[101/  6]\t\tLoss: 1.1082 Acc@1: 40.053%\n",
            "| Epoch [ 12/ 50] Iter[151/  6]\t\tLoss: 1.0755 Acc@1: 40.408%\n",
            "| Epoch [ 12/ 50] Iter[201/  6]\t\tLoss: 282.4364 Acc@1: 40.337%\n",
            "| Epoch [ 12/ 50] Iter[251/  6]\t\tLoss: 1.1005 Acc@1: 40.544%\n",
            "| Epoch [ 12/ 50] Iter[301/  6]\t\tLoss: 1.0760 Acc@1: 40.303%\n",
            "\n",
            "| Validation Epoch #12\t\t\tLoss: 1.0746 Acc@1: 40.52%\n",
            "lr = 0.001\n",
            "| Epoch [ 13/ 50] Iter[  1/  6]\t\tLoss: 1.0702 Acc@1: 42.188%\n",
            "| Epoch [ 13/ 50] Iter[ 51/  6]\t\tLoss: 30.8970 Acc@1: 40.839%\n",
            "| Epoch [ 13/ 50] Iter[101/  6]\t\tLoss: 1.1068 Acc@1: 39.898%\n",
            "| Epoch [ 13/ 50] Iter[151/  6]\t\tLoss: 2.2346 Acc@1: 40.335%\n",
            "| Epoch [ 13/ 50] Iter[201/  6]\t\tLoss: 1.0974 Acc@1: 40.539%\n",
            "| Epoch [ 13/ 50] Iter[251/  6]\t\tLoss: 1.0498 Acc@1: 40.476%\n",
            "| Epoch [ 13/ 50] Iter[301/  6]\t\tLoss: 1.1770 Acc@1: 40.251%\n",
            "\n",
            "| Validation Epoch #13\t\t\tLoss: 1.1059 Acc@1: 40.57%\n",
            "lr = 0.001\n",
            "| Epoch [ 14/ 50] Iter[  1/  6]\t\tLoss: 1.1203 Acc@1: 35.938%\n",
            "| Epoch [ 14/ 50] Iter[ 51/  6]\t\tLoss: 1.1544 Acc@1: 40.257%\n",
            "| Epoch [ 14/ 50] Iter[101/  6]\t\tLoss: 1.1080 Acc@1: 40.548%\n",
            "| Epoch [ 14/ 50] Iter[151/  6]\t\tLoss: 1.0503 Acc@1: 40.987%\n",
            "| Epoch [ 14/ 50] Iter[201/  6]\t\tLoss: 1.0799 Acc@1: 40.493%\n",
            "| Epoch [ 14/ 50] Iter[251/  6]\t\tLoss: 1.0822 Acc@1: 40.413%\n",
            "| Epoch [ 14/ 50] Iter[301/  6]\t\tLoss: 1.1006 Acc@1: 40.433%\n",
            "\n",
            "| Validation Epoch #14\t\t\tLoss: 1.0836 Acc@1: 40.57%\n",
            "lr = 0.001\n",
            "| Epoch [ 15/ 50] Iter[  1/  6]\t\tLoss: 1.0892 Acc@1: 42.188%\n",
            "| Epoch [ 15/ 50] Iter[ 51/  6]\t\tLoss: 1.1006 Acc@1: 41.728%\n",
            "| Epoch [ 15/ 50] Iter[101/  6]\t\tLoss: 1.0958 Acc@1: 41.600%\n",
            "| Epoch [ 15/ 50] Iter[151/  6]\t\tLoss: 1.0996 Acc@1: 41.370%\n",
            "| Epoch [ 15/ 50] Iter[201/  6]\t\tLoss: 1.1236 Acc@1: 41.325%\n",
            "| Epoch [ 15/ 50] Iter[251/  6]\t\tLoss: 1.0842 Acc@1: 40.706%\n",
            "| Epoch [ 15/ 50] Iter[301/  6]\t\tLoss: 1.0965 Acc@1: 40.599%\n",
            "\n",
            "| Validation Epoch #15\t\t\tLoss: 1.1031 Acc@1: 40.54%\n",
            "lr = 0.001\n",
            "| Epoch [ 16/ 50] Iter[  1/  6]\t\tLoss: 1.1632 Acc@1: 39.062%\n",
            "| Epoch [ 16/ 50] Iter[ 51/  6]\t\tLoss: 1.1140 Acc@1: 40.196%\n",
            "| Epoch [ 16/ 50] Iter[101/  6]\t\tLoss: 1.0903 Acc@1: 39.836%\n",
            "| Epoch [ 16/ 50] Iter[151/  6]\t\tLoss: 1.0919 Acc@1: 40.149%\n",
            "| Epoch [ 16/ 50] Iter[201/  6]\t\tLoss: 1.0851 Acc@1: 40.089%\n",
            "| Epoch [ 16/ 50] Iter[251/  6]\t\tLoss: 1.0922 Acc@1: 40.351%\n",
            "| Epoch [ 16/ 50] Iter[301/  6]\t\tLoss: 1.0995 Acc@1: 39.104%\n",
            "\n",
            "| Validation Epoch #16\t\t\tLoss: 1.1222 Acc@1: 31.02%\n",
            "lr = 0.001\n",
            "| Epoch [ 17/ 50] Iter[  1/  6]\t\tLoss: 2.5007 Acc@1: 28.125%\n",
            "| Epoch [ 17/ 50] Iter[ 51/  6]\t\tLoss: 1.0970 Acc@1: 30.362%\n",
            "| Epoch [ 17/ 50] Iter[101/  6]\t\tLoss: 1.1320 Acc@1: 30.585%\n",
            "| Epoch [ 17/ 50] Iter[151/  6]\t\tLoss: 1.0946 Acc@1: 33.764%\n",
            "| Epoch [ 17/ 50] Iter[201/  6]\t\tLoss: 1.1735 Acc@1: 35.533%\n",
            "| Epoch [ 17/ 50] Iter[251/  6]\t\tLoss: 1.1221 Acc@1: 36.510%\n",
            "| Epoch [ 17/ 50] Iter[301/  6]\t\tLoss: 1.0028 Acc@1: 36.898%\n",
            "\n",
            "| Validation Epoch #17\t\t\tLoss: 1.1089 Acc@1: 40.52%\n",
            "lr = 0.001\n",
            "| Epoch [ 18/ 50] Iter[  1/  6]\t\tLoss: 1.1139 Acc@1: 42.188%\n",
            "| Epoch [ 18/ 50] Iter[ 51/  6]\t\tLoss: 1.1004 Acc@1: 41.176%\n",
            "| Epoch [ 18/ 50] Iter[101/  6]\t\tLoss: 1.1032 Acc@1: 40.238%\n",
            "| Epoch [ 18/ 50] Iter[151/  6]\t\tLoss: 1.0723 Acc@1: 40.035%\n",
            "| Epoch [ 18/ 50] Iter[201/  6]\t\tLoss: 1.0894 Acc@1: 40.221%\n",
            "| Epoch [ 18/ 50] Iter[251/  6]\t\tLoss: 1.0604 Acc@1: 40.507%\n",
            "| Epoch [ 18/ 50] Iter[301/  6]\t\tLoss: 1.0946 Acc@1: 40.412%\n",
            "\n",
            "| Validation Epoch #18\t\t\tLoss: 1.0736 Acc@1: 40.55%\n",
            "lr = 0.001\n",
            "| Epoch [ 19/ 50] Iter[  1/  6]\t\tLoss: 1.0981 Acc@1: 46.875%\n",
            "| Epoch [ 19/ 50] Iter[ 51/  6]\t\tLoss: 1.1192 Acc@1: 29.259%\n",
            "| Epoch [ 19/ 50] Iter[101/  6]\t\tLoss: 1.1339 Acc@1: 28.620%\n",
            "| Epoch [ 19/ 50] Iter[151/  6]\t\tLoss: 1.0996 Acc@1: 28.456%\n",
            "| Epoch [ 19/ 50] Iter[201/  6]\t\tLoss: 1.1487 Acc@1: 28.483%\n",
            "| Epoch [ 19/ 50] Iter[251/  6]\t\tLoss: 1.1063 Acc@1: 28.947%\n",
            "| Epoch [ 19/ 50] Iter[301/  6]\t\tLoss: 1.0485 Acc@1: 31.079%\n",
            "\n",
            "| Validation Epoch #19\t\t\tLoss: 1.0628 Acc@1: 40.32%\n",
            "lr = 0.001\n",
            "| Epoch [ 20/ 50] Iter[  1/  6]\t\tLoss: 1.0946 Acc@1: 43.750%\n",
            "| Epoch [ 20/ 50] Iter[ 51/  6]\t\tLoss: 1.1541 Acc@1: 40.012%\n",
            "| Epoch [ 20/ 50] Iter[101/  6]\t\tLoss: 1.0598 Acc@1: 39.635%\n",
            "| Epoch [ 20/ 50] Iter[151/  6]\t\tLoss: 1.0852 Acc@1: 39.942%\n",
            "| Epoch [ 20/ 50] Iter[201/  6]\t\tLoss: 1.0590 Acc@1: 40.213%\n",
            "| Epoch [ 20/ 50] Iter[251/  6]\t\tLoss: 1.1348 Acc@1: 40.351%\n",
            "| Epoch [ 20/ 50] Iter[301/  6]\t\tLoss: 1.0892 Acc@1: 40.215%\n",
            "\n",
            "| Validation Epoch #20\t\t\tLoss: 1.1255 Acc@1: 40.54%\n",
            "lr = 0.001\n",
            "| Epoch [ 21/ 50] Iter[  1/  6]\t\tLoss: 1.0614 Acc@1: 40.625%\n",
            "| Epoch [ 21/ 50] Iter[ 51/  6]\t\tLoss: 1.0560 Acc@1: 40.227%\n",
            "| Epoch [ 21/ 50] Iter[101/  6]\t\tLoss: 1.2003 Acc@1: 39.527%\n",
            "| Epoch [ 21/ 50] Iter[151/  6]\t\tLoss: 1.0802 Acc@1: 36.331%\n",
            "| Epoch [ 21/ 50] Iter[201/  6]\t\tLoss: 1.1106 Acc@1: 35.308%\n",
            "| Epoch [ 21/ 50] Iter[251/  6]\t\tLoss: 1.1109 Acc@1: 34.344%\n",
            "| Epoch [ 21/ 50] Iter[301/  6]\t\tLoss: 1.0696 Acc@1: 33.949%\n",
            "\n",
            "| Validation Epoch #21\t\t\tLoss: 1.1418 Acc@1: 30.96%\n",
            "lr = 0.001\n",
            "| Epoch [ 22/ 50] Iter[  1/  6]\t\tLoss: 1.0655 Acc@1: 45.312%\n",
            "| Epoch [ 22/ 50] Iter[ 51/  6]\t\tLoss: 1.1014 Acc@1: 31.801%\n",
            "| Epoch [ 22/ 50] Iter[101/  6]\t\tLoss: 1.0921 Acc@1: 35.922%\n",
            "| Epoch [ 22/ 50] Iter[151/  6]\t\tLoss: 1.0679 Acc@1: 37.365%\n",
            "| Epoch [ 22/ 50] Iter[201/  6]\t\tLoss: 1.0595 Acc@1: 38.075%\n",
            "| Epoch [ 22/ 50] Iter[251/  6]\t\tLoss: 1.0719 Acc@1: 38.708%\n",
            "| Epoch [ 22/ 50] Iter[301/  6]\t\tLoss: 1.0979 Acc@1: 39.005%\n",
            "\n",
            "| Validation Epoch #22\t\t\tLoss: 1.1119 Acc@1: 40.55%\n",
            "lr = 0.001\n",
            "| Epoch [ 23/ 50] Iter[  1/  6]\t\tLoss: 1.1373 Acc@1: 35.938%\n",
            "| Epoch [ 23/ 50] Iter[ 51/  6]\t\tLoss: 1.1425 Acc@1: 39.185%\n",
            "| Epoch [ 23/ 50] Iter[101/  6]\t\tLoss: 1.0903 Acc@1: 40.130%\n",
            "| Epoch [ 23/ 50] Iter[151/  6]\t\tLoss: 1.0281 Acc@1: 40.159%\n",
            "| Epoch [ 23/ 50] Iter[201/  6]\t\tLoss: 1.0727 Acc@1: 40.252%\n",
            "| Epoch [ 23/ 50] Iter[251/  6]\t\tLoss: 1.1146 Acc@1: 40.220%\n",
            "| Epoch [ 23/ 50] Iter[301/  6]\t\tLoss: 1.0534 Acc@1: 40.241%\n",
            "\n",
            "| Validation Epoch #23\t\t\tLoss: 1.0960 Acc@1: 40.52%\n",
            "lr = 0.001\n",
            "| Epoch [ 24/ 50] Iter[  1/  6]\t\tLoss: 1.1092 Acc@1: 32.812%\n",
            "| Epoch [ 24/ 50] Iter[ 51/  6]\t\tLoss: 1.0868 Acc@1: 40.165%\n",
            "| Epoch [ 24/ 50] Iter[101/  6]\t\tLoss: 1.0725 Acc@1: 40.610%\n",
            "| Epoch [ 24/ 50] Iter[151/  6]\t\tLoss: 1.1001 Acc@1: 39.859%\n",
            "| Epoch [ 24/ 50] Iter[201/  6]\t\tLoss: 1.0807 Acc@1: 37.407%\n",
            "| Epoch [ 24/ 50] Iter[251/  6]\t\tLoss: 1.1138 Acc@1: 36.068%\n",
            "| Epoch [ 24/ 50] Iter[301/  6]\t\tLoss: 1.1278 Acc@1: 35.169%\n",
            "\n",
            "| Validation Epoch #24\t\t\tLoss: 1.2496 Acc@1: 30.95%\n",
            "lr = 0.001\n",
            "| Epoch [ 25/ 50] Iter[  1/  6]\t\tLoss: 1.1389 Acc@1: 25.000%\n",
            "| Epoch [ 25/ 50] Iter[ 51/  6]\t\tLoss: 1.0953 Acc@1: 30.760%\n",
            "| Epoch [ 25/ 50] Iter[101/  6]\t\tLoss: 1.1094 Acc@1: 30.987%\n",
            "| Epoch [ 25/ 50] Iter[151/  6]\t\tLoss: 1.0591 Acc@1: 33.113%\n",
            "| Epoch [ 25/ 50] Iter[201/  6]\t\tLoss: 1.1169 Acc@1: 34.818%\n",
            "| Epoch [ 25/ 50] Iter[251/  6]\t\tLoss: 1.1224 Acc@1: 36.056%\n",
            "| Epoch [ 25/ 50] Iter[301/  6]\t\tLoss: 1.0881 Acc@1: 36.618%\n",
            "\n",
            "| Validation Epoch #25\t\t\tLoss: 1.1545 Acc@1: 40.52%\n",
            "lr = 0.001\n",
            "| Epoch [ 26/ 50] Iter[  1/  6]\t\tLoss: 1.0660 Acc@1: 43.750%\n",
            "| Epoch [ 26/ 50] Iter[ 51/  6]\t\tLoss: 1.0995 Acc@1: 40.993%\n",
            "| Epoch [ 26/ 50] Iter[101/  6]\t\tLoss: 1.0698 Acc@1: 41.058%\n",
            "| Epoch [ 26/ 50] Iter[151/  6]\t\tLoss: 1.1193 Acc@1: 40.697%\n",
            "| Epoch [ 26/ 50] Iter[201/  6]\t\tLoss: 1.1142 Acc@1: 40.656%\n",
            "| Epoch [ 26/ 50] Iter[251/  6]\t\tLoss: 1.0658 Acc@1: 40.500%\n",
            "| Epoch [ 26/ 50] Iter[301/  6]\t\tLoss: 1.0753 Acc@1: 39.763%\n",
            "\n",
            "| Validation Epoch #26\t\t\tLoss: 1.0457 Acc@1: 28.46%\n",
            "lr = 0.001\n",
            "| Epoch [ 27/ 50] Iter[  1/  6]\t\tLoss: 1.1498 Acc@1: 23.438%\n",
            "| Epoch [ 27/ 50] Iter[ 51/  6]\t\tLoss: 1.0656 Acc@1: 28.339%\n",
            "| Epoch [ 27/ 50] Iter[101/  6]\t\tLoss: 3.1946 Acc@1: 28.342%\n",
            "| Epoch [ 27/ 50] Iter[151/  6]\t\tLoss: 1.1117 Acc@1: 28.580%\n",
            "| Epoch [ 27/ 50] Iter[201/  6]\t\tLoss: 1.1218 Acc@1: 28.786%\n",
            "| Epoch [ 27/ 50] Iter[251/  6]\t\tLoss: 1.1027 Acc@1: 30.073%\n",
            "| Epoch [ 27/ 50] Iter[301/  6]\t\tLoss: 1.1062 Acc@1: 31.754%\n",
            "\n",
            "| Validation Epoch #27\t\t\tLoss: 1.1007 Acc@1: 40.49%\n",
            "lr = 0.001\n",
            "| Epoch [ 28/ 50] Iter[  1/  6]\t\tLoss: 1.1221 Acc@1: 28.125%\n",
            "| Epoch [ 28/ 50] Iter[ 51/  6]\t\tLoss: 1.0858 Acc@1: 40.901%\n",
            "| Epoch [ 28/ 50] Iter[101/  6]\t\tLoss: 1.0649 Acc@1: 40.656%\n",
            "| Epoch [ 28/ 50] Iter[151/  6]\t\tLoss: 1.1043 Acc@1: 40.522%\n",
            "| Epoch [ 28/ 50] Iter[201/  6]\t\tLoss: 1.0457 Acc@1: 40.423%\n",
            "| Epoch [ 28/ 50] Iter[251/  6]\t\tLoss: 1.0713 Acc@1: 40.650%\n",
            "| Epoch [ 28/ 50] Iter[301/  6]\t\tLoss: 1.1360 Acc@1: 40.469%\n",
            "\n",
            "| Validation Epoch #28\t\t\tLoss: 0.9859 Acc@1: 40.54%\n",
            "lr = 0.001\n",
            "| Epoch [ 29/ 50] Iter[  1/  6]\t\tLoss: 1.0411 Acc@1: 45.312%\n",
            "| Epoch [ 29/ 50] Iter[ 51/  6]\t\tLoss: 1.1463 Acc@1: 40.196%\n",
            "| Epoch [ 29/ 50] Iter[101/  6]\t\tLoss: 1.0625 Acc@1: 40.718%\n",
            "| Epoch [ 29/ 50] Iter[151/  6]\t\tLoss: 1.0806 Acc@1: 40.677%\n",
            "| Epoch [ 29/ 50] Iter[201/  6]\t\tLoss: 1.1089 Acc@1: 40.672%\n",
            "| Epoch [ 29/ 50] Iter[251/  6]\t\tLoss: 1.0491 Acc@1: 40.550%\n",
            "| Epoch [ 29/ 50] Iter[301/  6]\t\tLoss: 1.0954 Acc@1: 40.386%\n",
            "\n",
            "| Validation Epoch #29\t\t\tLoss: 1.0841 Acc@1: 40.54%\n",
            "lr = 0.001\n",
            "| Epoch [ 30/ 50] Iter[  1/  6]\t\tLoss: 1.0747 Acc@1: 45.312%\n",
            "| Epoch [ 30/ 50] Iter[ 51/  6]\t\tLoss: 1.1070 Acc@1: 42.708%\n",
            "| Epoch [ 30/ 50] Iter[101/  6]\t\tLoss: 1.0949 Acc@1: 41.383%\n",
            "| Epoch [ 30/ 50] Iter[151/  6]\t\tLoss: 1.0844 Acc@1: 40.822%\n",
            "| Epoch [ 30/ 50] Iter[201/  6]\t\tLoss: 1.0966 Acc@1: 40.788%\n",
            "| Epoch [ 30/ 50] Iter[251/  6]\t\tLoss: 1.0757 Acc@1: 40.756%\n",
            "| Epoch [ 30/ 50] Iter[301/  6]\t\tLoss: 1.1767 Acc@1: 40.371%\n",
            "\n",
            "| Validation Epoch #30\t\t\tLoss: 1.0957 Acc@1: 40.55%\n",
            "lr = 0.001\n",
            "| Epoch [ 31/ 50] Iter[  1/  6]\t\tLoss: 1.1003 Acc@1: 40.625%\n",
            "| Epoch [ 31/ 50] Iter[ 51/  6]\t\tLoss: 1.1413 Acc@1: 39.645%\n",
            "| Epoch [ 31/ 50] Iter[101/  6]\t\tLoss: 1.1730 Acc@1: 39.511%\n",
            "| Epoch [ 31/ 50] Iter[151/  6]\t\tLoss: 1.1339 Acc@1: 39.445%\n",
            "| Epoch [ 31/ 50] Iter[201/  6]\t\tLoss: 1.1433 Acc@1: 39.840%\n",
            "| Epoch [ 31/ 50] Iter[251/  6]\t\tLoss: 1.0628 Acc@1: 40.289%\n",
            "| Epoch [ 31/ 50] Iter[301/  6]\t\tLoss: 1.1195 Acc@1: 40.397%\n",
            "\n",
            "| Validation Epoch #31\t\t\tLoss: 1.0882 Acc@1: 40.55%\n",
            "lr = 0.001\n",
            "| Epoch [ 32/ 50] Iter[  1/  6]\t\tLoss: 1.1370 Acc@1: 37.500%\n",
            "| Epoch [ 32/ 50] Iter[ 51/  6]\t\tLoss: 1.1289 Acc@1: 39.216%\n",
            "| Epoch [ 32/ 50] Iter[101/  6]\t\tLoss: 1.1371 Acc@1: 39.821%\n",
            "| Epoch [ 32/ 50] Iter[151/  6]\t\tLoss: 1.1018 Acc@1: 40.190%\n",
            "| Epoch [ 32/ 50] Iter[201/  6]\t\tLoss: 1.0752 Acc@1: 40.571%\n",
            "| Epoch [ 32/ 50] Iter[251/  6]\t\tLoss: 1.1419 Acc@1: 40.370%\n",
            "| Epoch [ 32/ 50] Iter[301/  6]\t\tLoss: 1.0918 Acc@1: 39.182%\n",
            "\n",
            "| Validation Epoch #32\t\t\tLoss: 1.0783 Acc@1: 30.99%\n",
            "lr = 0.001\n",
            "| Epoch [ 33/ 50] Iter[  1/  6]\t\tLoss: 1.0535 Acc@1: 40.625%\n",
            "| Epoch [ 33/ 50] Iter[ 51/  6]\t\tLoss: 1.1485 Acc@1: 31.587%\n",
            "| Epoch [ 33/ 50] Iter[101/  6]\t\tLoss: 1.1275 Acc@1: 31.791%\n",
            "| Epoch [ 33/ 50] Iter[151/  6]\t\tLoss: 1.0919 Acc@1: 32.067%\n",
            "| Epoch [ 33/ 50] Iter[201/  6]\t\tLoss: 1.0549 Acc@1: 31.996%\n",
            "| Epoch [ 33/ 50] Iter[251/  6]\t\tLoss: 1.1161 Acc@1: 32.084%\n",
            "| Epoch [ 33/ 50] Iter[301/  6]\t\tLoss: 1.1111 Acc@1: 33.607%\n",
            "\n",
            "| Validation Epoch #33\t\t\tLoss: 1.0162 Acc@1: 40.57%\n",
            "lr = 0.001\n",
            "| Epoch [ 34/ 50] Iter[  1/  6]\t\tLoss: 1.0677 Acc@1: 46.875%\n",
            "| Epoch [ 34/ 50] Iter[ 51/  6]\t\tLoss: 1.0609 Acc@1: 40.135%\n",
            "| Epoch [ 34/ 50] Iter[101/  6]\t\tLoss: 1.0631 Acc@1: 40.238%\n",
            "| Epoch [ 34/ 50] Iter[151/  6]\t\tLoss: 1.1032 Acc@1: 40.170%\n",
            "| Epoch [ 34/ 50] Iter[201/  6]\t\tLoss: 1.0946 Acc@1: 40.306%\n",
            "| Epoch [ 34/ 50] Iter[251/  6]\t\tLoss: 1.0947 Acc@1: 40.388%\n",
            "| Epoch [ 34/ 50] Iter[301/  6]\t\tLoss: 1.0841 Acc@1: 40.298%\n",
            "\n",
            "| Validation Epoch #34\t\t\tLoss: 1.1163 Acc@1: 40.55%\n",
            "lr = 0.001\n",
            "| Epoch [ 35/ 50] Iter[  1/  6]\t\tLoss: 1.0938 Acc@1: 43.750%\n",
            "| Epoch [ 35/ 50] Iter[ 51/  6]\t\tLoss: 1.1021 Acc@1: 41.605%\n",
            "| Epoch [ 35/ 50] Iter[101/  6]\t\tLoss: 1.0954 Acc@1: 36.293%\n",
            "| Epoch [ 35/ 50] Iter[151/  6]\t\tLoss: 1.1137 Acc@1: 33.889%\n",
            "| Epoch [ 35/ 50] Iter[201/  6]\t\tLoss: 1.0987 Acc@1: 32.478%\n",
            "| Epoch [ 35/ 50] Iter[251/  6]\t\tLoss: 1.0997 Acc@1: 31.773%\n",
            "| Epoch [ 35/ 50] Iter[301/  6]\t\tLoss: 1.1846 Acc@1: 31.245%\n",
            "\n",
            "| Validation Epoch #35\t\t\tLoss: 1.0933 Acc@1: 28.46%\n",
            "lr = 0.001\n",
            "| Epoch [ 36/ 50] Iter[  1/  6]\t\tLoss: 1.1336 Acc@1: 23.438%\n",
            "| Epoch [ 36/ 50] Iter[ 51/  6]\t\tLoss: 1.1156 Acc@1: 29.504%\n",
            "| Epoch [ 36/ 50] Iter[101/  6]\t\tLoss: 1.1295 Acc@1: 35.133%\n",
            "| Epoch [ 36/ 50] Iter[151/  6]\t\tLoss: 1.1281 Acc@1: 37.376%\n",
            "| Epoch [ 36/ 50] Iter[201/  6]\t\tLoss: 1.0859 Acc@1: 37.873%\n",
            "| Epoch [ 36/ 50] Iter[251/  6]\t\tLoss: 1.0681 Acc@1: 38.309%\n",
            "| Epoch [ 36/ 50] Iter[301/  6]\t\tLoss: 1.0980 Acc@1: 38.689%\n",
            "\n",
            "| Validation Epoch #36\t\t\tLoss: 1.0928 Acc@1: 40.51%\n",
            "lr = 0.001\n",
            "| Epoch [ 37/ 50] Iter[  1/  6]\t\tLoss: 1.5973 Acc@1: 40.625%\n",
            "| Epoch [ 37/ 50] Iter[ 51/  6]\t\tLoss: 1.0040 Acc@1: 41.452%\n",
            "| Epoch [ 37/ 50] Iter[101/  6]\t\tLoss: 1.0924 Acc@1: 40.656%\n",
            "| Epoch [ 37/ 50] Iter[151/  6]\t\tLoss: 1.1659 Acc@1: 40.480%\n",
            "| Epoch [ 37/ 50] Iter[201/  6]\t\tLoss: 1.1424 Acc@1: 40.524%\n",
            "| Epoch [ 37/ 50] Iter[251/  6]\t\tLoss: 1.0943 Acc@1: 38.621%\n",
            "| Epoch [ 37/ 50] Iter[301/  6]\t\tLoss: 1.0572 Acc@1: 37.401%\n",
            "\n",
            "| Validation Epoch #37\t\t\tLoss: 1.1092 Acc@1: 30.95%\n",
            "lr = 0.001\n",
            "| Epoch [ 38/ 50] Iter[  1/  6]\t\tLoss: 1.0657 Acc@1: 31.250%\n",
            "| Epoch [ 38/ 50] Iter[ 51/  6]\t\tLoss: 1.0660 Acc@1: 33.517%\n",
            "| Epoch [ 38/ 50] Iter[101/  6]\t\tLoss: 1.1166 Acc@1: 33.199%\n",
            "| Epoch [ 38/ 50] Iter[151/  6]\t\tLoss: 1.1201 Acc@1: 32.585%\n",
            "| Epoch [ 38/ 50] Iter[201/  6]\t\tLoss: 1.1127 Acc@1: 31.919%\n",
            "| Epoch [ 38/ 50] Iter[251/  6]\t\tLoss: 1.0889 Acc@1: 31.518%\n",
            "| Epoch [ 38/ 50] Iter[301/  6]\t\tLoss: 1.0644 Acc@1: 33.197%\n",
            "\n",
            "| Validation Epoch #38\t\t\tLoss: 1.0740 Acc@1: 40.54%\n",
            "lr = 0.001\n",
            "| Epoch [ 39/ 50] Iter[  1/  6]\t\tLoss: 1.0929 Acc@1: 43.750%\n",
            "| Epoch [ 39/ 50] Iter[ 51/  6]\t\tLoss: 1.0737 Acc@1: 41.667%\n",
            "| Epoch [ 39/ 50] Iter[101/  6]\t\tLoss: 1.1390 Acc@1: 40.903%\n",
            "| Epoch [ 39/ 50] Iter[151/  6]\t\tLoss: 1.1238 Acc@1: 40.408%\n",
            "| Epoch [ 39/ 50] Iter[201/  6]\t\tLoss: 1.0790 Acc@1: 40.431%\n",
            "| Epoch [ 39/ 50] Iter[251/  6]\t\tLoss: 1.0788 Acc@1: 40.370%\n",
            "| Epoch [ 39/ 50] Iter[301/  6]\t\tLoss: 1.1003 Acc@1: 40.397%\n",
            "\n",
            "| Validation Epoch #39\t\t\tLoss: 1.0730 Acc@1: 40.54%\n",
            "lr = 0.001\n",
            "| Epoch [ 40/ 50] Iter[  1/  6]\t\tLoss: 1.0740 Acc@1: 56.250%\n",
            "| Epoch [ 40/ 50] Iter[ 51/  6]\t\tLoss: 1.1437 Acc@1: 40.043%\n",
            "| Epoch [ 40/ 50] Iter[101/  6]\t\tLoss: 1.0760 Acc@1: 40.610%\n",
            "| Epoch [ 40/ 50] Iter[151/  6]\t\tLoss: 1.1440 Acc@1: 37.852%\n",
            "| Epoch [ 40/ 50] Iter[201/  6]\t\tLoss: 1.1451 Acc@1: 35.650%\n",
            "| Epoch [ 40/ 50] Iter[251/  6]\t\tLoss: 1.1552 Acc@1: 34.120%\n",
            "| Epoch [ 40/ 50] Iter[301/  6]\t\tLoss: 1.0921 Acc@1: 33.124%\n",
            "\n",
            "| Validation Epoch #40\t\t\tLoss: 1.0770 Acc@1: 28.46%\n",
            "lr = 0.001\n",
            "| Epoch [ 41/ 50] Iter[  1/  6]\t\tLoss: 1.0959 Acc@1: 31.250%\n",
            "| Epoch [ 41/ 50] Iter[ 51/  6]\t\tLoss: 1.0918 Acc@1: 30.576%\n",
            "| Epoch [ 41/ 50] Iter[101/  6]\t\tLoss: 1.0893 Acc@1: 34.653%\n",
            "| Epoch [ 41/ 50] Iter[151/  6]\t\tLoss: 1.0912 Acc@1: 36.496%\n",
            "| Epoch [ 41/ 50] Iter[201/  6]\t\tLoss: 1.0786 Acc@1: 37.648%\n",
            "| Epoch [ 41/ 50] Iter[251/  6]\t\tLoss: 1.1047 Acc@1: 38.353%\n",
            "| Epoch [ 41/ 50] Iter[301/  6]\t\tLoss: 1.1314 Acc@1: 38.819%\n",
            "\n",
            "| Validation Epoch #41\t\t\tLoss: 1.0665 Acc@1: 40.55%\n",
            "lr = 0.001\n",
            "| Epoch [ 42/ 50] Iter[  1/  6]\t\tLoss: 1.0958 Acc@1: 46.875%\n",
            "| Epoch [ 42/ 50] Iter[ 51/  6]\t\tLoss: 1.1085 Acc@1: 40.502%\n",
            "| Epoch [ 42/ 50] Iter[101/  6]\t\tLoss: 1.0424 Acc@1: 40.842%\n",
            "| Epoch [ 42/ 50] Iter[151/  6]\t\tLoss: 1.0211 Acc@1: 40.325%\n",
            "| Epoch [ 42/ 50] Iter[201/  6]\t\tLoss: 1.1329 Acc@1: 40.446%\n",
            "| Epoch [ 42/ 50] Iter[251/  6]\t\tLoss: 1.1007 Acc@1: 40.345%\n",
            "| Epoch [ 42/ 50] Iter[301/  6]\t\tLoss: 1.1080 Acc@1: 40.371%\n",
            "\n",
            "| Validation Epoch #42\t\t\tLoss: 87.7711 Acc@1: 40.55%\n",
            "lr = 0.001\n",
            "| Epoch [ 43/ 50] Iter[  1/  6]\t\tLoss: 1.1272 Acc@1: 42.188%\n",
            "| Epoch [ 43/ 50] Iter[ 51/  6]\t\tLoss: 1.1180 Acc@1: 40.411%\n",
            "| Epoch [ 43/ 50] Iter[101/  6]\t\tLoss: 1.0596 Acc@1: 40.393%\n",
            "| Epoch [ 43/ 50] Iter[151/  6]\t\tLoss: 1.1376 Acc@1: 40.377%\n",
            "| Epoch [ 43/ 50] Iter[201/  6]\t\tLoss: 1.1146 Acc@1: 40.275%\n",
            "| Epoch [ 43/ 50] Iter[251/  6]\t\tLoss: 1.0830 Acc@1: 40.233%\n",
            "| Epoch [ 43/ 50] Iter[301/  6]\t\tLoss: 1.0782 Acc@1: 40.438%\n",
            "\n",
            "| Validation Epoch #43\t\t\tLoss: 1.0912 Acc@1: 40.55%\n",
            "lr = 0.001\n",
            "| Epoch [ 44/ 50] Iter[  1/  6]\t\tLoss: 1.0912 Acc@1: 39.062%\n",
            "| Epoch [ 44/ 50] Iter[ 51/  6]\t\tLoss: 1.1023 Acc@1: 41.728%\n",
            "| Epoch [ 44/ 50] Iter[101/  6]\t\tLoss: 1.0874 Acc@1: 40.579%\n",
            "| Epoch [ 44/ 50] Iter[151/  6]\t\tLoss: 1.0973 Acc@1: 40.418%\n",
            "| Epoch [ 44/ 50] Iter[201/  6]\t\tLoss: 1.1289 Acc@1: 40.283%\n",
            "| Epoch [ 44/ 50] Iter[251/  6]\t\tLoss: 1.1262 Acc@1: 40.351%\n",
            "| Epoch [ 44/ 50] Iter[301/  6]\t\tLoss: 1.1196 Acc@1: 40.371%\n",
            "\n",
            "| Validation Epoch #44\t\t\tLoss: 1.0711 Acc@1: 40.57%\n",
            "lr = 0.001\n",
            "| Epoch [ 45/ 50] Iter[  1/  6]\t\tLoss: 1.0968 Acc@1: 35.938%\n",
            "| Epoch [ 45/ 50] Iter[ 51/  6]\t\tLoss: 1.0887 Acc@1: 40.993%\n",
            "| Epoch [ 45/ 50] Iter[101/  6]\t\tLoss: 1.0705 Acc@1: 40.625%\n",
            "| Epoch [ 45/ 50] Iter[151/  6]\t\tLoss: 1.1438 Acc@1: 40.408%\n",
            "| Epoch [ 45/ 50] Iter[201/  6]\t\tLoss: 1.1027 Acc@1: 40.252%\n",
            "| Epoch [ 45/ 50] Iter[251/  6]\t\tLoss: 1.0539 Acc@1: 40.364%\n",
            "| Epoch [ 45/ 50] Iter[301/  6]\t\tLoss: 1.1157 Acc@1: 40.459%\n",
            "\n",
            "| Validation Epoch #45\t\t\tLoss: 1.1615 Acc@1: 40.57%\n",
            "lr = 0.001\n",
            "| Epoch [ 46/ 50] Iter[  1/  6]\t\tLoss: 1.1506 Acc@1: 37.500%\n",
            "| Epoch [ 46/ 50] Iter[ 51/  6]\t\tLoss: 1.1475 Acc@1: 39.767%\n",
            "| Epoch [ 46/ 50] Iter[101/  6]\t\tLoss: 1.0655 Acc@1: 40.068%\n",
            "| Epoch [ 46/ 50] Iter[151/  6]\t\tLoss: 1.0995 Acc@1: 40.066%\n",
            "| Epoch [ 46/ 50] Iter[201/  6]\t\tLoss: 1.0852 Acc@1: 40.368%\n",
            "| Epoch [ 46/ 50] Iter[251/  6]\t\tLoss: 1.0891 Acc@1: 40.195%\n",
            "| Epoch [ 46/ 50] Iter[301/  6]\t\tLoss: 1.0889 Acc@1: 38.735%\n",
            "\n",
            "| Validation Epoch #46\t\t\tLoss: 1.1328 Acc@1: 30.96%\n",
            "lr = 0.001\n",
            "| Epoch [ 47/ 50] Iter[  1/  6]\t\tLoss: 1.0890 Acc@1: 37.500%\n",
            "| Epoch [ 47/ 50] Iter[ 51/  6]\t\tLoss: 1.0787 Acc@1: 32.138%\n",
            "| Epoch [ 47/ 50] Iter[101/  6]\t\tLoss: 1.1394 Acc@1: 31.482%\n",
            "| Epoch [ 47/ 50] Iter[151/  6]\t\tLoss: 1.1350 Acc@1: 31.664%\n",
            "| Epoch [ 47/ 50] Iter[201/  6]\t\tLoss: 1.1402 Acc@1: 31.514%\n",
            "| Epoch [ 47/ 50] Iter[251/  6]\t\tLoss: 1.1083 Acc@1: 31.325%\n",
            "| Epoch [ 47/ 50] Iter[301/  6]\t\tLoss: 1.1077 Acc@1: 32.023%\n",
            "\n",
            "| Validation Epoch #47\t\t\tLoss: 0.9977 Acc@1: 40.55%\n",
            "lr = 0.001\n",
            "| Epoch [ 48/ 50] Iter[  1/  6]\t\tLoss: 1.1438 Acc@1: 29.688%\n",
            "| Epoch [ 48/ 50] Iter[ 51/  6]\t\tLoss: 1.1587 Acc@1: 39.737%\n",
            "| Epoch [ 48/ 50] Iter[101/  6]\t\tLoss: 1.0614 Acc@1: 40.362%\n",
            "| Epoch [ 48/ 50] Iter[151/  6]\t\tLoss: 1.1386 Acc@1: 40.263%\n",
            "| Epoch [ 48/ 50] Iter[201/  6]\t\tLoss: 1.1110 Acc@1: 40.019%\n",
            "| Epoch [ 48/ 50] Iter[251/  6]\t\tLoss: 1.1446 Acc@1: 40.202%\n",
            "| Epoch [ 48/ 50] Iter[301/  6]\t\tLoss: 1.1165 Acc@1: 40.355%\n",
            "\n",
            "| Validation Epoch #48\t\t\tLoss: 1.2102 Acc@1: 40.55%\n",
            "lr = 0.001\n",
            "| Epoch [ 49/ 50] Iter[  1/  6]\t\tLoss: 1.0740 Acc@1: 45.312%\n",
            "| Epoch [ 49/ 50] Iter[ 51/  6]\t\tLoss: 1.0967 Acc@1: 40.993%\n",
            "| Epoch [ 49/ 50] Iter[101/  6]\t\tLoss: 1.0582 Acc@1: 40.501%\n",
            "| Epoch [ 49/ 50] Iter[151/  6]\t\tLoss: 1.0966 Acc@1: 40.366%\n",
            "| Epoch [ 49/ 50] Iter[201/  6]\t\tLoss: 1.1076 Acc@1: 40.539%\n",
            "| Epoch [ 49/ 50] Iter[251/  6]\t\tLoss: 1.1093 Acc@1: 38.123%\n",
            "| Epoch [ 49/ 50] Iter[301/  6]\t\tLoss: 1.1037 Acc@1: 36.545%\n",
            "\n",
            "| Validation Epoch #49\t\t\tLoss: 1.1054 Acc@1: 28.47%\n",
            "lr = 0.001\n",
            "MLP Training Done!\n"
          ]
        }
      ],
      "source": [
        "## QUESTION 2\n",
        "## MLP Model for sentiment analysis\n",
        "## All the libraries are already imported\n",
        "## We will use the same train and test data\n",
        "\n",
        "# Define the MLP model\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(MLPClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Initialize the MLP model\n",
        "input_size = train_embeddings.size(1)  # Dimension of the embeddings\n",
        "hidden_size = 256  # You can adjust this value\n",
        "num_classes = NUM_CLASSES\n",
        "\n",
        "mlp_model = MLPClassifier(input_size, hidden_size, num_classes).to(device)\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = torch.optim.Adam(mlp_model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training function for MLP\n",
        "def train_mlp(model, train_loader, optimizer, epoch, log_interval=50):\n",
        "    model.train()\n",
        "    loss_cpu = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, data in enumerate(train_loader, 0):\n",
        "        inputs, target = data['input_ids'], data['label']\n",
        "        inputs, target = inputs.cuda(), target.cuda()\n",
        "        inputs = inputs.detach()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        loss_cpu += loss.item()\n",
        "        total += target.size(0)\n",
        "        correct += predicted.eq(target.data).cpu().sum()\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%'\n",
        "                  % (epoch, EPOCHS, batch_idx + 1,\n",
        "                     (len(train_loader) // BATCH_SIZE) + 1, loss.item(), 100. * correct / total))\n",
        "\n",
        "    return loss_cpu / len(train_loader)\n",
        "\n",
        "# Testing function for MLP\n",
        "def test_mlp(model, test_loader, epoch):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(test_loader, 0):\n",
        "            inputs, target = data['input_ids'], data['label']\n",
        "            inputs, target = inputs.cuda(), target.cuda()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, target)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target.data).cpu().sum()\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    test_loss = test_loss / len(test_loader)\n",
        "    print(\"\\n| Validation Epoch #%d\\t\\t\\tLoss: %.4f Acc@1: %.2f%%\" % (epoch, loss.item(), 100. * correct / total))\n",
        "    return test_loss, 100. * correct / total\n",
        "\n",
        "# Train and evaluate the MLP model\n",
        "print(\"Training MLP model.\")\n",
        "train_history = []\n",
        "val_history = []\n",
        "best_error = float('inf')\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    loss = train_mlp(mlp_model, trainloader, optimizer, epoch)\n",
        "    train_history.append(loss)\n",
        "    val_loss = test_mlp(mlp_model, valloader, epoch)\n",
        "    val_history.append(val_loss[0])\n",
        "    lr_scheduler.step(val_loss[0])\n",
        "    print('lr =', get_lr(optimizer))\n",
        "\n",
        "    if val_loss[0] < best_error:\n",
        "        best_error = val_loss[0]\n",
        "        print('Best RMSE is of : ' + str(best_error), 'for epoch :', epoch, 'ERROR TEST = ', val_loss)\n",
        "        mlp_model.load_state_dict(mlp_model.state_dict(), strict=True)\n",
        "\n",
        "print(\"MLP Training Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 989445,
          "sourceId": 1808590,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30761,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
